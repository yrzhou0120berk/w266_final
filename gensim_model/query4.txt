Query: unique binary search tree

************************** NEXT RESULT **************************************
## @package mobile_exporter
# Module caffe2.python.mobile_exporter

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
from caffe2.python import core, utils
from caffe2.proto import caffe2_pb2
import numpy as np


def add_tensor(net, name, blob):
    ''' Create an operator to store the tensor 'blob',
        run the operator to put the blob to workspace.
        uint8 is stored as an array of string with one element.
    '''
    kTypeNameMapper = {
        np.dtype('float32'): "GivenTensorFill",
        np.dtype('int32'): "GivenTensorIntFill",
        np.dtype('int64'): "GivenTensorInt64Fill",
        np.dtype('uint8'): "GivenTensorStringFill",
    }

    shape = blob.shape
    values = blob
    # pass array of uint8 as a string to save storage
    # storing uint8_t has a large overhead for now
    if blob.dtype == np.dtype('uint8'):
        shape = [1]
        values = [str(blob.data)]

    op = core.CreateOperator(
        kTypeNameMapper[blob.dtype],
        [], [name],
        arg=[
            utils.MakeArgument("shape", shape),
            utils.MakeArgument("values", values),
        ]
    )
    net.op.extend([op])


def Export(workspace, net, params):
    """Returns init_net and predict_net suitable for writing to disk
       and loading into a Predictor"""
    proto = net if isinstance(net, caffe2_pb2.NetDef) else net.Proto()
    predict_net = caffe2_pb2.NetDef()
    predict_net.CopyFrom(proto)
    init_net = caffe2_pb2.NetDef()
    # Populate the init_net.
    ssa, blob_versions = core.get_ssa(net)
    inputs = []
    for versioned_inputs, _ in ssa:
        inputs += [name for name, _ in versioned_inputs]

    input_blobs = [blob_name for blob_name, version in
                   blob_versions.items()
                   if version == 0 and blob_name not in params]
    # Blobs that are never used as an input to another layer,
    # i.e. strictly output blobs.
    output_blobs = [blob_name for blob_name, version in
                    blob_versions.items()
                    if version != 0 and blob_name not in inputs]

    for blob_ref in params:
        blob_name = str(blob_ref)
        blob = workspace.FetchBlob(blob_name)
        add_tensor(init_net, blob_name, blob)
    # We have to make sure the blob exists in the namespace
    # and we can do so with fake data. (Which is immediately overwritten
    # by any typical usage)
    for blob_name in input_blobs:
        init_net.op.extend(
            [
                core.CreateOperator(
                    "GivenTensorFill", [], [blob_name],
                    arg=[
                        utils.MakeArgument("shape", [1, 1]),
                        utils.MakeArgument("values", [0.0])
                    ]
                )
            ]
        )

    # Now we make input/output_blobs line up with what Predictor expects.
    del predict_net.external_input[:]
    predict_net.external_input.extend(input_blobs)
    # For populating weights
    predict_net.external_input.extend(proto.external_input)
    # Ensure the output is also consistent with what we want
    del predict_net.external_output[:]
    predict_net.external_output.extend(output_blobs)
    return init_net, predict_net

Query: unique binary search tree

************************** NEXT RESULT **************************************
"""
Setup pubsub for the kwargs message protocol. In a default installation
this is the default protocol so this module is only needed if setupkargs
utility functions are used, or in a custom installation where kwargs 
is not the default messaging protocol (such as in some versions of 
wxPython). 

This module must be imported before the first ``from pubsub import pub``
statement in the application. Once :mod:pub has been imported, the messaging 
protocol cannot be changed (i.e., importing it after the first 
``from pubsub import pub`` statement has undefined behavior). 
"""

"""
:copyright: Copyright since 2006 by Oliver Schoenborn, all rights reserved.
:license: BSD, see LICENSE_BSD_Simple.txt for details.
"""

from . import policies
policies.msgDataProtocol = 'kwargs'


def transitionFromArg1(commonName):
    """Utility function to assist migrating an application from using 
    the arg1 messaging protocol to using the kwargs protocol. Call this 
    after having run and debugged your application with ``setuparg1.enforceArgName(commonName)``. See the migration docs
    for more detais. 
    """
    policies.setMsgDataArgName(2, commonName)

Query: unique binary search tree

************************** NEXT RESULT **************************************
__author__ = 'Charlie'
# Utils used with tensorflow implemetation
import tensorflow as tf
import numpy as np
import os, sys
from six.moves import urllib
import tarfile
import zipfile
from skimage import io, color
import scipy.io


def maybe_download_and_extract(dir_path, url_name, is_tarfile=False, is_zipfile=False):
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
    filename = url_name.split('/')[-1]
    filepath = os.path.join(dir_path, filename)
    if not os.path.exists(filepath):
        def _progress(count, block_size, total_size):
            sys.stdout.write(
                '\r>> Downloading %s %.1f%%' % (filename, float(count * block_size) / float(total_size) * 100.0))
            sys.stdout.flush()

        filepath, _ = urllib.request.urlretrieve(url_name, filepath, reporthook=_progress)
        print()
        statinfo = os.stat(filepath)
        print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')
        if is_tarfile:
            tarfile.open(filepath, 'r:gz').extractall(dir_path)
        elif is_zipfile:
            with zipfile.ZipFile(filepath) as zf:
                zip_dir = zf.namelist()[0]
                zf.extractall(dir_path)


def get_model_data(dir_path, model_url):
    maybe_download_and_extract(dir_path, model_url)
    filename = model_url.split("/")[-1]
    filepath = os.path.join(dir_path, filename)
    if not os.path.exists(filepath):
        raise IOError("VGG Model not found!")
    data = scipy.io.loadmat(filepath)
    return data


def save_image(image, save_dir, name):
    """
    Save image by unprocessing and converting to rgb.
    :param image: iamge to save
    :param save_dir: location to save image at
    :param name: prefix to save filename
    :return:
    """
    image = color.lab2rgb(image)
    io.imsave(os.path.join(save_dir, name + ".png"), image)

def get_variable_set_not_trainable(weights, name):
    init = tf.constant_initializer(weights, dtype=tf.float32)
    var = tf.get_variable(name=name, initializer=init, shape=weights.shape, trainable=False)
    return var
def weight_variable_not_trainable(shape, stddev=0.02, name=None):
    initial = tf.truncated_normal(shape, stddev=stddev)
    if name is None:
        return tf.Variable(initial, trainable=False)
    else:
        return tf.get_variable(name, initializer=initial, trainable=False)
def bias_variable_not_trainable(shape, name=None):
    initial = tf.constant(0.0, shape=shape)
    if name is None:
        return tf.Variable(initial, trainable=False)
    else:
        return tf.get_variable(name, initializer=initial, trainable=False)

    

def get_variable(weights, name):
    init = tf.constant_initializer(weights, dtype=tf.float32)
    var = tf.get_variable(name=name, initializer=init, shape=weights.shape)
    return var


def weight_variable(shape, stddev=0.02, name=None):
    initial = tf.truncated_normal(shape, stddev=stddev)
    if name is None:
        return tf.Variable(initial)
    else:
        return tf.get_variable(name, initializer=initial)


def bias_variable(shape, name=None):
    initial = tf.constant(0.0, shape=shape)
    if name is None:
        return tf.Variable(initial)
    else:
        return tf.get_variable(name, initializer=initial)

    
    
    

def get_tensor_size(tensor):
    from operator import mul
    return reduce(mul, (d.value for d in tensor.get_shape()), 1)


def conv2d_basic(x, W, bias):
    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding="SAME")
    return tf.nn.bias_add(conv, bias)


def conv2d_strided(x, W, b):
    conv = tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding="SAME")
    return tf.nn.bias_add(conv, b)


def conv2d_transpose_strided(x, W, b, output_shape=None, stride=2):
    # print x.get_shape()
    # print W.get_shape()
    if output_shape is None:
        output_shape = x.get_shape().as_list()
        output_shape[1] *= 2
        output_shape[2] *= 2
        output_shape[3] = W.get_shape().as_list()[2]
    # print output_shape
    conv = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding="SAME")
    return tf.nn.bias_add(conv, b)


def leaky_relu(x, alpha=0.2, name=""):
    return tf.maximum(alpha * x, x, name)


def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")


def avg_pool_2x2(x):
    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME")


def batch_norm(x, n_out, phase_train, scope='bn', decay=0.9, eps=1e-5, stddev=0.02):
    """
    Code taken from http://stackoverflow.com/a/34634291/2267819
    """
    with tf.variable_scope(scope):
        beta = tf.get_variable(name='beta', shape=[n_out], initializer=tf.constant_initializer(0.0)
                               , trainable=True)
        gamma = tf.get_variable(name='gamma', shape=[n_out], initializer=tf.random_normal_initializer(1.0, stddev),
                                trainable=True)
        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')
        ema = tf.train.ExponentialMovingAverage(decay=decay)

        def mean_var_with_update():
            ema_apply_op = ema.apply([batch_mean, batch_var])
            with tf.control_dependencies([ema_apply_op]):
                return tf.identity(batch_mean), tf.identity(batch_var)

        mean, var = tf.cond(phase_train,
                            mean_var_with_update,
                            lambda: (ema.average(batch_mean), ema.average(batch_var)))
        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)
    return normed


def process_image(image, mean_pixel):
    return image - mean_pixel


def unprocess_image(image, mean_pixel):
    return image + mean_pixel


def add_to_regularization_and_summary(var):
    if var is not None:
        tf.summary.histogram(var.op.name, var)
        tf.add_to_collection("reg_loss", tf.nn.l2_loss(var))


def add_activation_summary(var):
    tf.summary.histogram(var.op.name + "/activation", var)
    tf.scalar_summary(var.op.name + "/sparsity", tf.nn.zero_fraction(var))


def add_gradient_summary(grad, var):
    if grad is not None:
        tf.summary.histogram(var.op.name + "/gradient", grad)


"""
The residual code below is taken and modified
 from https://github.com/tensorflow/models/blob/master/resnet/resnet_model.py
"""


def residual_block(x, in_filter, out_filter, stride, phase_train, is_conv=True, leakiness=0.0,
                   activate_before_residual=False):
    """Residual unit with 2 sub layers."""
    if activate_before_residual:
        with tf.variable_scope('shared_activation'):
            x = batch_norm(x, out_filter, phase_train, scope="init_bn")
            x = leaky_relu(x, alpha=leakiness, name="lrelu")
            orig_x = x
    else:
        with tf.variable_scope('residual_only_activation'):
            orig_x = x
            x = batch_norm(x, out_filter, phase_train, scope="init_bn")
            x = leaky_relu(x, alpha=leakiness, name="lrelu")

    with tf.variable_scope('sub1'):
        if is_conv:
            x = conv_no_bias('conv1', x, 3, in_filter, out_filter, stride)
        else:
            x = conv_transpose_no_bias('conv_t1', x, 3, in_filter, out_filter, stride)

    with tf.variable_scope('sub2'):
        x = batch_norm(x, out_filter, phase_train, scope="bn2")
        x = tf.nn.relu(x, "relu")
        if is_conv:
            x = conv_no_bias('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])
        else:
            x = conv_transpose_no_bias('conv_t2', x, 3, in_filter, in_filter, [1, 1, 1, 1])

    with tf.variable_scope('sub_add'):
        if in_filter != out_filter:
            if is_conv:
                orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')
            else:
                orig_x = tf.nn.fractional_avg_pool(orig_x, stride)  # Available only in tf 0.11 - not tested
            orig_x = tf.pad(
                orig_x, [[0, 0], [0, 0], [0, 0],
                         [(out_filter - in_filter) // 2, (out_filter - in_filter) // 2]])
        x += orig_x

    # tf.logging.info('image after unit %s', x.get_shape())
    return x


def bottleneck_residual_block(x, in_filter, out_filter, stride, phase_train, is_conv=True, leakiness=0.0,
                              activate_before_residual=False):
    """Bottleneck resisual unit with 3 sub layers."""
    if activate_before_residual:
        with tf.variable_scope('common_bn_relu'):
            x = batch_norm(x, out_filter, phase_train, scope="init_bn")
            x = leaky_relu(x, alpha=leakiness, name="lrelu")
            orig_x = x
    else:
        with tf.variable_scope('residual_bn_relu'):
            orig_x = x
            x = batch_norm(x, out_filter, phase_train, scope="init_bn")
            x = leaky_relu(x, alpha=leakiness, name="lrelu")

    with tf.variable_scope('sub1'):
        if is_conv:
            x = conv_no_bias('conv1', x, 1, in_filter, out_filter / 4, stride)
        else:
            x = conv_transpose_no_bias('conv_t1', x, 1, out_filter / 4, out_filter, stride)

    with tf.variable_scope('sub2'):
        x = batch_norm(x, out_filter, phase_train, scope="bn2")
        x = leaky_relu(x, alpha=leakiness, name="lrelu")
        if is_conv:
            x = conv_no_bias('conv2', x, 3, out_filter / 4, out_filter / 4, [1, 1, 1, 1])
        else:
            x = conv_transpose_no_bias('conv_t2', x, 3, out_filter / 4, out_filter / 4, [1, 1, 1, 1])

    with tf.variable_scope('sub3'):
        x = batch_norm(x, out_filter, phase_train, scope="bn3")
        x = leaky_relu(x, alpha=leakiness, name="lrelu")
        if is_conv:
            x = conv_no_bias('conv3', x, 1, out_filter / 4, out_filter, [1, 1, 1, 1])
        else:
            x = conv_transpose_no_bias('conv_t3', x, 1, in_filter, out_filter / 4, [1, 1, 1, 1])

    with tf.variable_scope('sub_add'):
        if in_filter != out_filter:
            if is_conv:
                orig_x = conv_no_bias('project', orig_x, 1, in_filter, out_filter, stride)
            else:
                orig_x = conv_transpose_no_bias('project', orig_x, 1, in_filter, out_filter, stride)
        x += orig_x

    # tf.logging.info('image after unit %s', x.get_shape())
    return x


def conv_no_bias(name, x, filter_size, in_filters, out_filters, strides):
    """Convolution."""
    with tf.variable_scope(name):
        n = filter_size * filter_size * out_filters
        kernel = tf.get_variable(
            'DW', [filter_size, filter_size, in_filters, out_filters],
            tf.float32, initializer=tf.random_normal_initializer(
                stddev=np.sqrt(2.0 / n)))
        return tf.nn.conv2d(x, kernel, strides, padding='SAME')


def conv_transpose_no_bias(name, x, filter_size, in_filters, out_filters, strides):
    """Convolution Transpose."""
    with tf.variable_scope(name):
        n = filter_size * filter_size * out_filters
        kernel = tf.get_variable(
            'DW', [filter_size, filter_size, in_filters, out_filters],
            tf.float32, initializer=tf.random_normal_initializer(
                stddev=np.sqrt(2.0 / n)))

        output_shape = tf.shape(x)
        output_shape[1] *= strides[1]
        output_shape[2] *= strides[2]
        output_shape[3] = in_filters

        return tf.nn.conv2d_transpose(x, kernel, output_shape=output_shape, strides=strides, padding='SAME')

Query: unique binary search tree

************************** NEXT RESULT **************************************
# Time:  O(n), n is the number of the integers.
# Space: O(h), h is the depth of the nested lists.

# """
# This is the interface that allows for creating nested lists.
# You should not implement it, or speculate about its implementation
# """
#class NestedInteger(object):
#    def isInteger(self):
#        """
#        @return True if this NestedInteger holds a single integer, rather than a nested list.
#        :rtype bool
#        """
#
#    def getInteger(self):
#        """
#        @return the single integer that this NestedInteger holds, if it holds a single integer
#        Return None if this NestedInteger holds a nested list
#        :rtype int
#        """
#
#    def getList(self):
#        """
#        @return the nested list that this NestedInteger holds, if it holds a nested list
#        Return None if this NestedInteger holds a single integer
#        :rtype List[NestedInteger]
#        """

class NestedIterator(object):

    def __init__(self, nestedList):
        """
        Initialize your data structure here.
        :type nestedList: List[NestedInteger]
        """
        self.__depth = [[nestedList, 0]]


    def next(self):
        """
        :rtype: int
        """
        nestedList, i = self.__depth[-1]
        self.__depth[-1][1] += 1
        return nestedList[i].getInteger()


    def hasNext(self):
        """
        :rtype: bool
        """
        while self.__depth:
            nestedList, i = self.__depth[-1]
            if i == len(nestedList):
                self.__depth.pop()
            elif nestedList[i].isInteger():
                    return True
            else:
                self.__depth[-1][1] += 1
                self.__depth.append([nestedList[i].getList(), 0])
        return False


# Your NestedIterator object will be instantiated and called as such:
# i, v = NestedIterator(nestedList), []
# while i.hasNext(): v.append(i.next())

Query: unique binary search tree

************************** NEXT RESULT **************************************
#!/usr/bin/env python
# encoding: utf-8

from __future__ import absolute_import, unicode_literals, print_function

import sys
import re

from .text_exporter import TextExporter
from ..util import WARNING_COLOR, ERROR_COLOR, RESET_COLOR


class PrjctExporter(TextExporter):
    """This Exporter can convert entries and journals into Markdown formatted
    text with front matter usable by the Ablog extention for Sphinx."""
    names = ["prjct"]
    extension = "md"

    @classmethod
    def export_entry(cls, entry, to_multifile=True):
        """Returns a markdown representation of a single entry, with Ablog front matter."""
        if to_multifile is False:
            print("{}ERROR{}: Prjct export must be to individual files. Please \
                specify a directory to export to.".format(ERROR_COLOR, RESET_COLOR, file=sys.stderr))
            return

        date_str = entry.date.strftime(entry.journal.config['timeformat'])
        body_wrapper = "\n" if entry.body else ""
        body = body_wrapper + entry.body

        tagsymbols = entry.journal.config['tagsymbols']
        # see also Entry.Entry.rag_regex
        multi_tag_regex = re.compile(r'(?u)^\s*([{tags}][-+*#/\w]+\s*)+$'.format(tags=tagsymbols), re.UNICODE)

        newbody = ''
        for line in body.splitlines(True):
            if multi_tag_regex.match(line):
                """Tag only lines"""
                line = ''
            newbody = newbody + line

        # pass headings as is

        if len(entry.tags) > 0:
            tags_str = '   :tags: ' + ', '.join([tag[1:] for tag in entry.tags]) + '\n'
        else:
            tags_str = ''

        if hasattr(entry, 'location'):
            location_str = '   :location: {}\n'.format(entry.location.get('Locality', ''))
        else:
            location_str = ''

        # source directory is  entry.journal.config['journal']
        # output directory is...?

        return "# {title}\n\n```eval_rst\n.. post:: {date}\n{tags}{category}{author}{location}{language}```\n\n{body}{space}" \
            .format(
                date=date_str,
                title=entry.title,
                tags=tags_str,
                category="   :category: jrnl\n",
                author="",
                location=location_str,
                language="",
                body=newbody,
                space="\n"
            )

    @classmethod
    def export_journal(cls, journal):
        """Returns an error, as Prjct export requires a directory as a target."""
        print("{}ERROR{}: Prjct export must be to individual files. \
            Please specify a directory to export to.".format(ERROR_COLOR, RESET_COLOR), file=sys.stderr)
        return

Query: unique binary search tree

************************** NEXT RESULT **************************************
'''

@author: MengLai
Updater: YeTian
'''
import os
import tempfile
import uuid
import time

import zstackwoodpecker.test_util as test_util
import zstackwoodpecker.test_lib as test_lib
import zstackwoodpecker.test_state as test_state
import zstacklib.utils.ssh as ssh

test_stub = test_lib.lib_get_test_stub()
test_obj_dict = test_state.TestStateDict()
tmp_file = '/tmp/%s' % uuid.uuid1().get_hex()
vm_inv = None


def test():
    global vm_inv
    test_util.test_dsc('Create test vm to test zstack upgrade by -u.')
    image_name = os.environ.get('imageName_i_c7_z_1.5')
    iso_path = os.environ.get('iso_path')
    iso_19_path = os.environ.get('iso_19_path')
    iso_10_path = os.environ.get('iso_10_path')
    iso_20_path = os.environ.get('iso_20_path')
    iso_21_path = os.environ.get('iso_21_path')
    iso_230_path = os.environ.get('iso_230_path')

    zstack_latest_version = os.environ.get('zstackLatestVersion')
    zstack_latest_path = os.environ.get('zstackLatestInstaller')
    vm_name = os.environ.get('vmName')
    upgrade_script_path = os.environ.get('upgradeScript')

    vm_inv = test_stub.create_vm_scenario(image_name, vm_name)
    vm_ip = vm_inv.vmNics[0].ip
    test_lib.lib_wait_target_up(vm_ip, 22)

    test_stub.make_ssh_no_password(vm_ip, tmp_file)


    test_util.test_dsc('Update MN IP')
    test_stub.update_mn_hostname(vm_ip, tmp_file)
    test_stub.update_mn_ip(vm_ip, tmp_file)
    test_stub.reset_rabbitmq(vm_ip, tmp_file)
    test_stub.start_mn(vm_ip, tmp_file)
    test_stub.check_installation(vm_ip, tmp_file)

    test_stub.update_19_iso(vm_ip, tmp_file, iso_19_path, upgrade_script_path)

    #pkg_num = 1.6
    release_ver=['1.6','1.7','1.8','1.9','1.10','2.0.0','2.1.0','2.2.0','2.3.0','2.3.1']
    curren_num = float(os.environ.get('releasePkgNum'))
    #while pkg_num <= curren_num:
    for pkg_num in release_ver:
        test_util.test_logger('Upgrade zstack to %s' % pkg_num)
	#if str(pkg_num) == '1.7':
        #    test_stub.update_19_iso(vm_ip, tmp_file, iso_19_path, upgrade_script_path)
        if str(pkg_num) == '1.10':
            test_stub.update_10_iso(vm_ip, tmp_file, iso_10_path, upgrade_script_path)
        if str(pkg_num) == '2.0.0':
            test_stub.update_20_iso(vm_ip, tmp_file, iso_20_path, upgrade_script_path)
        if str(pkg_num) == '2.1.0':
            test_stub.update_21_iso(vm_ip, tmp_file, iso_21_path, upgrade_script_path)
        if str(pkg_num) == '2.3.0':
            test_stub.update_230_iso(vm_ip, tmp_file, iso_230_path, upgrade_script_path)
        upgrade_pkg = os.environ.get('zstackPkg_%s' % pkg_num)
        test_stub.upgrade_zstack(vm_ip, upgrade_pkg, tmp_file) 
        test_stub.start_mn(vm_ip, tmp_file)
        test_stub.check_zstack_version(vm_ip, tmp_file, str(pkg_num))
        #test_stub.check_installation(vm_ip, tmp_file)
        #pkg_num = pkg_num + 0.1

    test_util.test_dsc('Upgrade zstack to latest') 

    test_stub.update_iso(vm_ip, tmp_file, iso_path, upgrade_script_path)
    test_stub.upgrade_zstack(vm_ip, zstack_latest_path, tmp_file) 
    test_stub.start_mn(vm_ip, tmp_file)
    test_stub.check_zstack_version(vm_ip, tmp_file, zstack_latest_version)
    test_stub.check_installation(vm_ip, tmp_file)

    os.system('rm -f %s' % tmp_file)
    test_stub.destroy_vm_scenario(vm_inv.uuid)
    test_util.test_pass('ZStack upgrade Test Success')

#Will be called only if exception happens in test().
def error_cleanup():
    global vm_inv
    os.system('rm -f %s' % tmp_file)
    test_stub.destroy_vm_scenario(vm_inv.uuid)

Query: unique binary search tree

************************** NEXT RESULT **************************************
from docutils import nodes
from docutils.parsers.rst import Directive, directives

try:
    import micawber
except ImportError:
    micawber = None  # NOQA


class Embed(Directive):
    """ Restructured text extension for inserting any
        sort of media using micawber."""
    has_content = False
    required_arguments = 1
    optional_arguments = 999

    def get_providers(self):
        return micawber.bootstrap_noembed()

    def run(self):
        if micawber is None:
            msg = "To use the embed directive, install micawber first."
            return [nodes.raw('', '<div class="text-error">{0}</div>'.format(msg), format='html')]
        url = " ".join(self.arguments)
        html = micawber.parse_text(url, self.get_providers())
        return [nodes.raw('', html, format='html')]


def register_directive():
    directives.register_directive('embed', Embed)

Query: unique binary search tree

************************** NEXT RESULT **************************************
from copy import deepcopy

def wrap(value):
    """
    Wraps the given value in a Document or DocumentList as applicable.
    """
    if isinstance(value, Document) or isinstance(value, DocumentList):
        return value
    elif isinstance(value, dict):
        return Document(value)
    elif isinstance(value, list):
        return DocumentList(value)
    else:
        return value


def unwrap(value):
    """
    Unwraps the given Document or DocumentList as applicable.
    """
    if isinstance(value, Document):
        return value.to_dict()
    elif isinstance(value, DocumentList):
        return value.to_list()
    else:
        return value


class ChangeTracker(object):
    def __init__(self, instance):
        self._instance = instance
        self.reset_changes()

    def reset_changes(self):
        """
        Resets the document's internal change-tracking state. All field additions,
        changes and deletions are forgotten.
        """
        self._added = []
        self._previous = {}
        self._deleted = {}

    def update(self, other):
        self.reset_changes()
        self._added.extend(other._added)
        self._previous.update(other._previous)
        self._deleted.update(other._deleted)

    def note_change(self, key, value):
        """
        Updates change state to reflect a change to a field. Takes care of ignoring
        no-ops, reversions and takes appropriate steps if the field was previously
        deleted or added to ensure the change state purely reflects the diff since
        last reset.
        """
        # If we're changing the value and we haven't done so already, note it.
        if value != self._instance[key] and key not in self._previous and key not in self._added:
            self._previous[key] = self._instance[key]

        # If we're setting the value back to the original value, discard the change note
        if key in self._previous and value == self._previous[key]:
            del self._previous[key]

    def note_addition(self, key, value):
        """
        Updates the change state to reflect the addition of a field. Detects previous
        changes and deletions of the field and acts accordingly.
        """
        # If we're adding a field we previously deleted, remove the deleted note.
        if key in self._deleted:
            # If the key we're adding back has a different value, then it's a change
            if value != self._deleted[key]:
                self._previous[key] = self._deleted[key]
            del self._deleted[key]
        else:
            self._added.append(key)

    def note_deletion(self, key):
        """
        Notes the deletion of a field.
        """
        # If we'rew deleting a key we previously added, then there is no diff
        if key in self._added:
            self._added.remove(key)
        else:
            # If the deleted key was previously changed, use the original value
            if key in self._previous:
                self._deleted[key] = self._previous[key]
                del self._previous[key]
            else:
                self._deleted[key] = self._instance[key]

    @property
    def changed(self):
        """
        Returns a dict containing just the fields which have changed on this
        Document since it was created or last saved, together with their new
        values.

            doc['name']             # => 'bob'
            doc['name'] = 'clive'
            doc.changed             # => {'name': 'clive'}
        """
        return {key: self._instance[key] for key in self._previous}

    @property
    def changes(self):
        """
        Returns a dict containing just the fields which have changed on this
        Document since it was created or last saved, together with both their
        previous and current values

            doc['name']             # => 'bob'
            doc['name'] = 'clive'
            doc.changes             # => {'name': ('bob', clive')}
        """
        return {key: (self._previous[key], self._instance[key])
                for key in self._previous}

    @property
    def added(self):
        """
            doc                     # => {'name': 'bob'}
            doc['age'] = 42
            doc.added               # => {'age': 42}
        """
        return {key: self._instance[key] for key in self._added}

    @property
    def deleted(self):
        """
            doc                     # => {'name': 'bob'}
            del doc['name']
            doc.deleted             # => {'name': 'bob'}
        """
        return self._deleted

class Document(dict):
    """
    Subclass of dict which adds some useful functionality around change tracking.
    """

    def __init__(self, initial=None, **kwargs):
        if initial:
            self.update(initial, **kwargs)
        elif kwargs:
            self.update(**kwargs)
        self.reset_changes()

    @property
    def _tracker(self):
        try:
            return self._change_tracker
        except AttributeError:
            self._change_tracker = ChangeTracker(self)
            return self._change_tracker

    def reset_changes(self):
        self._tracker.reset_changes()

    def reset_all_changes(self):
        """
        Resets change tracking in this document, recursing into child Documents and
        DocumentLists.
        """
        self.reset_changes()
        for value in self.values():
            if isinstance(value, Document) or isinstance(value, DocumentList):
                value.reset_all_changes()

    def __deepcopy__(self, memo):
        clone = type(self)(deepcopy(dict(self), memo))
        clone._tracker.update(self._tracker)
        return clone

    @property
    def changed(self):
        return self._tracker.changed

    @property
    def changes(self):
        return self._tracker.changes

    @property
    def added(self):
        return self._tracker.added

    @property
    def deleted(self):
        return self._tracker.deleted

    def __setitem__(self, key, value):
        if key in self:
            self._tracker.note_change(key, value)
        else:
            self._tracker.note_addition(key, value)
        super(Document, self).__setitem__(key, wrap(value))

    def __delitem__(self, key):
        self._tracker.note_deletion(key)
        super(Document, self).__delitem__(key)

    def update(self, other=None, **kwargs):
        if other:
            for key in other:
                self[key] = other[key]

        if kwargs:
            for key, value in kwargs.iteritems():
                self[key] = value

    def setdefault(self, key, default):
        return super(Document, self).setdefault(key, wrap(default))

    def populate(self, other):
        """Like update, but clears the contents first."""
        self.clear()
        self.update(other)
        self.reset_all_changes()

    def to_dict(self):
        """
        Returns the contents of the Document as a raw dict. Also recurses
        into child Documents and DocumentLists converting those to dicts
        and lists respectively.
        """
        return {key: unwrap(value) for key, value in self.iteritems()}


class DocumentList(list):
    """
    Subclass of list which provides some additional details around change tracking.
    """
    def __init__(self, initial=None):
        if initial:
            self.extend(initial)

    def reset_all_changes(self):
        for value in self:
            if isinstance(value, Document) or isinstance(value, DocumentList):
                value.reset_all_changes()

    def __setslice__(self, i, j, sequence):
        super(DocumentList, self).__setslice__(i, j, [wrap(value) for value in sequence])

    def __setitem__(self, index, value):
        super(DocumentList, self).__setitem__(index, wrap(value))

    def extend(self, other):
        super(DocumentList, self).extend([wrap(value) for value in other])

    def append(self, item):
        super(DocumentList, self).append(wrap(item))

    def insert(self, i, item):
        super(DocumentList, self).insert(i, wrap(item))

    def remove(self, item):
        super(DocumentList, self).remove(item)

    def pop(self, *args):
        return super(DocumentList, self).pop(*args)

    def to_list(self):
        """
        Returns the contents of the DocumentList as a raw list. Also recurses
        into child Documents and DocumentLists converting those to dicts
        and lists respectively.
        """
        return [unwrap(value) for value in self]

Query: unique binary search tree

************************** NEXT RESULT **************************************
#!/usr/bin/env python
# licensed under the CreativeCommons, Share-Alike, Attribution (CC-BY-SA from creativecommons.com)
# email: dreamingforward@gmail.com
 
"""Bag types:  a set-like container that counts the number of same items rather than consuming memory for storage."""

import random  #pick()

_DEBUG = True

class IntegerBag(dict):
    """Implements a bag type that allows item counts to be negative."""

    __slots__ = []

    def __init__(self, init={}):
        """Initialize bag with optional contents.

        >>> b = Bag()   #creates empty bag
        >>> b
        {}
        >>> print(IntegerBag({1: -1, 2: 0, 3: -9}))
        {1: -1, 3: -9}

        Can initialize with (key, count) list as in standard dict.
        However, duplicate keys will accumulate counts:
        >>> print(Bag([(1, 2), (2, 4), (1, 7)]))
        {1: 9, 2: 4}
        """
        if not init or isinstance(init, self.__class__):
            dict.__init__(self, init)   #values known to be good, use faster dict creation
        else:   #initializing with list or plain dict
            dict.__init__(self)
            if isinstance(init, dict):
                for key, count in init.items():
                    self[key] = count #will test invariants
            else:       #sequence may contain duplicates, so add to existing value, if any
                for key, count in init:
                    self[key] += count

    def fromkeys(cls, iterable, count=1):
        """Class method which creates bag from iterable adding optional count for each item.

        >>> b = Bag({'b': 2, 'c': 1, 'a': 3})
        >>> b2 = Bag.fromkeys(['a', 'b', 'c', 'b', 'a', 'a'])
        >>> b3 = Bag.fromkeys("abacab")
        >>> assert b == b2 == b3

        >>> word_count = Bag.fromkeys("how much wood could a wood chuck chuck".split())
        >>> print(word_count)
        {'a': 1, 'chuck': 2, 'could': 1, 'how': 1, 'much': 1, 'wood': 2}

        An optional count can be specified.  Count added each time item is encountered.
        >>> print(Bag.fromkeys("abacab", 5))
        {'a': 15, 'b': 10, 'c': 5}
        """
        b = cls()
        for key in iterable: #could just return b.__iadd__(iterable, count)
            b[key] += count  #perhaps slower than necessary but will simplify derived classes that override __setitem__()
        return b

    fromkeys = classmethod(fromkeys)

    def update(self, items, count=1):
        """Adds contents to bag from other mapping type or iterable.

        >>> ib = IntegerBag.fromkeys('abc')
        >>> ib.update({'a': 2, 'b': 1, 'c': 0})
        >>> print(ib)
        {'a': 3, 'b': 2, 'c': 1}

        Negative updates are allowable.
        >>> ib.update({'a': -2, 'b': -2, 'c': -2, 'd': 2})
        >>> print(ib)
        {'a': 1, 'c': -1, 'd': 2}

        Can call with iterable.  Amount added can be specified by count parameter:
        >>> ib.update(['a','b'], 2)
        >>> print(ib)
        {'a': 3, 'b': 2, 'c': -1, 'd': 2}

        Values that can't be converted to ints are skipped and will raise TypeError.
        >>> ib.update({0: 'test1', 'a': 'test2', 'd': 3, 'f': '1.0', 'c': 2.0})
        Traceback (most recent call last):
        TypeError: unsupported operand type(s) for +=: 'int' and 'str'
        >>> print(ib)
        {'a': 3, 'b': 2, 'c': 1, 'd': 5}

        Updating Bag with values that would cause the count to go negative
        sets count to 0, removing item.
        >>> b = Bag({'a': 1, 'c': 2, 'd': 5})
        >>> b.update({'a': -4, 'b': -1, 'c': -2, 'd': -2})
        >>> print(b)
        {'d': 3}

        NOTE:  Exceptions are only reported on the last bad element encountered.
        """
        #XXX may be able to improve this by calling dict methods directly and/or using map and operator functions
        #XXX should raise exception and return unchanged self if problem encountered!
        #XXX or use logging.warning() and continue
        err = False
        if isinstance(items, dict):
            for key, count in items.items():
                try:
                    self[key] += count  #may be slower than necessary
                except TypeError as error: err = True #FIXME should have to re-assign to propagate error:  check docs
        else: #sequence
            for key in items:
                try:
                    self[key] += count
                except TypeError as error: err = Trie
        if err: raise TypeError(error)

    def pick(self, count=1, remove=True): #XXX perhaps better to default to False?
        """Returns a bag with 'count' random items from bag (defaults to 1), removing the items unless told otherwise.

        >>> b = IntegerBag({'a': 3, 'b': -2, 'c': 1})
        >>> sub = b.pick(4)
        >>> sub.size, b.size
        (4, 2)
        """
        l = list(self.itereach())
        picked = IntegerBag(random.sample(l, min(abs(count), len(l))))
        if count < 0:  picked *= (-1)  #this probably not useful except for Network class
        if remove: self -= picked
        return picked

    def pop(self, item):
        """Remove all of item from bag, returning its count, if any.

        >>> b = IntegerBag.fromkeys("abacab")
        >>> b.pop('b')
        2
        >>> b.pop('z')
        0
        >>> print(b)
        {'a': 3, 'c': 1}
        """
        return super(IntegerBag, self).pop(item, 0)

    def discard(self, item):
        """Removes all of the specified item if it exists, otherwise ignored.

        >>> b = Bag.fromkeys("abacab")
        >>> b.discard('b')
        >>> b.discard('d')  #non-existent items ignored
        >>> print(b)
        {'a': 3, 'c': 1}
        """
        try: del self[item] #note: this does not call __getitem__
        except KeyError: pass

    def setdefault(self, item, count=1):
        count = self._filter(count)
        return count and dict.setdefault(self, item, count)

    def itereach(self):  #XXX consider rename akin to Python3 rules
        """Will iterate through all items in bag individually.

        >>> b = Bag.fromkeys("abacab")
        >>> l = list(b.itereach()); l.sort()
        >>> l
        [('a', 1), ('a', 1), ('a', 1), ('b', 1), ('b', 1), ('c', 1)]
        >>> b = IntegerBag(b)
        >>> b['b'] = -2
        >>> l = list(b.itereach()); l.sort()
        >>> l
        [('a', 1), ('a', 1), ('a', 1), ('b', -1), ('b', -1), ('c', 1)]

        Note: iteration on bag itself just iterates through unique keys:
        >>> l = list(b) ; l.sort()
        >>> l
        ['a', 'b', 'c']
        """
        for key, count in self.items():
            for i in range(abs(count)):
                yield (key, count >= 0 and 1 or -1) #consider returning (key, +/-1) pair to account for negative counts

    def __iadd__(self, other):
        """Add items in bag.

        >>> b = Bag()
        >>> b += [1, 2, 1, 0]
        >>> print(b)
        {0: 1, 1: 2, 2: 1}
        >>> b.clear()
        >>> b += "abca"
        >>> print(b)
        {'a': 2, 'b': 1, 'c': 1}
        """
        self.update(other, 1)  #XXX may fail mid-update...
        return self

    def __add__(self, other):  #XXX better way to create copy?? (in case self.__class__ has more complicated constructor...)
        """Add one bag to another, returns type of first bag.

        >>> b = IntegerBag({1: 2, 2: -2}) + Bag({1: 5, 2: 1, 3: 7})
        >>> b, "IntegerBag" in str(type(b))
        ({1: 7, 2: -1, 3: 7}, True)
        """
        return self.__class__(self).__iadd__(other)

    def __isub__(self, other):
        """Subtract items from bag.

        >>> b = Bag.fromkeys("abacab")
        >>> b -= "cccccab"
        >>> print(b)
        {'a': 2, 'b': 1}
        """
        if isinstance(other, dict):
            other = IntegerBag(other) * (-1)
        self.update(other, -1)
        return self

    def __sub__(self, other):
        """Subtract items from bag.

        >>> IntegerBag({1: 2, 2: -2}) - {1: 5, 2: -2, 3: 7}
        {1: -3, 3: -7}
        """
        return self.__class__(self).__isub__(other)

    def __imul__(self, factor):
        """Multiply bag contents by factor.

        >>> b = Bag.fromkeys("abacab")
        >>> b *= 4
        >>> print(b)
        {'a': 12, 'b': 8, 'c': 4}

        Negative factors can be used with IntegerBag.
        >>> ib = IntegerBag(b)
        >>> ib *= -1
        >>> print(ib)
        {'a': -12, 'b': -8, 'c': -4}

        Trying that on a Bag will return empty bag (akin to list behavior).
        >>> b *= -1
        >>> b
        {}

        Zero factors will return empty bag.
        >>> b += "abacab"
        >>> b *= 0
        >>> b
        {}

        """
        if self._filter(factor):
            for item, count in self.items():
                dict.__setitem__(self, item, count*factor) #bypass test logic in bag.__setitem__
        else:   #factor==0 or negative on Bag
            dict.clear(self)    #call dict.clear to protect subclass which might override and do other things besides clear dict values
        return self

    def __mul__(self, factor):
        """Returns new bag of same type multiplied by factor.

        >>> d = {1: 2, 2: 4, 3: -9}
        >>> IntegerBag(d) * -1
        {1: -2, 2: -4, 3: 9}
        >>> Bag(d) * -1
        {}
        """
        #XXX should perhaps use explicit IntBag in case derived class needs parameters -- or use copy()???
        return self.__class__(self).__imul__(factor)

    def _size(self):
        """Returns sum of absolute value of item counts in bag.

        >>> b = IntegerBag.fromkeys("abacab")
        >>> b['a'] = -4
        >>> b.size
        7
        """
        return sum(map(abs, self.values()))

    size = property(_size, None, None, "Sum of absolute count values in the bag")

    def __getitem__(self, item):
        """Returns total count for given item, or zero if item not in bag.

        >>> b = Bag.fromkeys("abacab")
        >>> b['a']
        3
        >>> b['d']
        0
        """
        return self.get(item, 0)

    count = __getitem__

    def __setitem__(self, item, count):
        """Sets the count for the given item in bag, removing if zero.

        >>> b = Bag()
        >>> b[1] = 3
        >>> b[3] = 1.6  #floats get coerced to ints
        >>> b[4] = "2"  #as do int strings
        >>> print(b)
        {1: 3, 3: 1, 4: 2}

        If count is zero, all 'matching items' are deleted from bag.
        >>> b[2] = 0
        >>> print(b)
        {1: 3, 3: 1, 4: 2}

        Counts for IntegerBag are allowed to be negative.
        >>> ib = IntegerBag(b)
        >>> ib[4] = -2
        >>> ib[5] -= 2
        >>> ib[1] -= 4
        >>> ib[3] -= 1
        >>> print(ib)
        {1: -1, 4: -2, 5: -2}

        Trying to set negative values on Bag reverts to zero.
        >>> b[4] = -2
        >>> b[4]
        0

        If count is non-integer, an exception is raised.
        >>> b[1] = "oops"  #doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ValueError: invalid literal for int(): oops
        """
        count = self._filter(count)
        if count:
            dict.__setitem__(self, item, count)  #XXX should this call super instead of dict (for cases of multiple inheritence etc...)
        else:   #setting to 0 so discard key
            self.discard(item)

    def __str__(self):
        """Convert self to string with items in sorted order.

        >>> str(IntegerBag())
        '{}'
        >>> str(IntegerBag({'b': -2, 'a': 3, 'c': 1, 1: 0}))
        "{'a': 3, 'b': -2, 'c': 1}"
        >>> str(Bag.fromkeys("abacab"))
        "{'a': 3, 'b': 2, 'c': 1}"
        """
        #sort by values, largest first? should we sort at all?
        if _DEBUG: self._validate()
        if not self: return '{}'    #nothing to sort
        keys = sorted(self) #this extra assigment necessary???  !Must remember basic python!...
        return '{%s}' % ', '.join(["%r: %r" % (k, self[k]) for k in keys])

    def _filter(value): #XXX could just set _filter = int but doctest complains even under Python 2.3.3
        """Coerces value to int and returns it, or raise raises TypeError."""
        return int(value)

    _filter = staticmethod(_filter)

    def _validate(self):
        """Check class invariants.

        >>> b = IntegerBag.fromkeys("abc")
        >>> dict.__setitem__(b, 'a', "oops")
        >>> b._validate() #doctest: +IGNORE_EXCEPTION_DETAIL
        Traceback (most recent call last):
        ValueError: invalid literal for int(): oops
        >>> b = Bag()
        >>> dict.__setitem__(b, 'a', 0)    #zero values are normally deleted
        >>> b
        {'a': 0}
        >>> b._validate()
        Traceback (most recent call last):
        AssertionError: zero value encountered
        >>> b = Bag()
        >>> dict.__setitem__(b, 'a', -1)   #negative values not allowed
        >>> b._validate()
        Traceback (most recent call last):
        AssertionError: unfiltered value
        """
        for count in self.values():
            assert count == self._filter(count), "unfiltered value"
            assert count, "zero value encountered"


class Bag(IntegerBag):
    """Standard bag class.  Allows only non-negative bag counts."""

    __slots__ = []

    def _size(self):
        """Returns total number of items in bag.

        >>> b = Bag.fromkeys("abacab")
        >>> b.size
        6
        """
        return sum(self.values())

    size = property(_size, None, None, "Sum of all bag values.")

    def _filter(value):
        """Returns 0 if value is negative. """
        return max(int(value), 0)

    _filter = staticmethod(_filter)


def _test():
    """Miscillaneous tests:

    Equality test.  Can compare against dictionary or bag types.
    >>> Bag.fromkeys("abacab") == {'a': 3, 'b': 2, 'c': 1}
    True
    >>> b, l = Bag.fromkeys([1, 2, 1, 3, 1]), [1, 1, 1, 3, 2]
    >>> b == l
    False
    >>> b == Bag.fromkeys(l) == IntegerBag.fromkeys(l)
    True

    Tests for non-zero:
    >>> b = Bag()
    >>> bool(b)
    False
    >>> b += [0]
    >>> bool(b)
    True
    """
    pass

if __name__ == "__main__":
    import doctest
    return doctest.testmod()

Query: unique binary search tree

************************** NEXT RESULT **************************************
'''
Hello again and welcome to another python 3 basics video. In this video we
will be covering default function parameters
'''

# so normally, you write a function like so:
def simple(num1,num2):
    pass

# What you can do, however is:

def simple(num1, num2=5):
    # what this does is specify a "default parameter" just in case
    # one is not specified.
    # this is useful so all parameters dont need to be called
    # every single time. Generally, this is used for modules.
    # an example would be a module that makes windows for users.
    pass


# so here, the user must specifiy width and height, but a font of times
# new roman, for example, is the default so they dont have to say that
# every single time.

def basic_window(width,height,font='TNR'):
    # let us just print out everything
    print(width,height,font)



# now, when we call basic_window, we can break a rule we established
# earlier:

# see, only two parameters, when we require 3
basic_window(350,500)

# we can do this because there is a default if font is not specified. Should
# a user wish to specify however, they can do

basic_window(350,500,font='courier')

# here, it is just important that you place any parameters with default values
# at the very end, to avoid trouble when calling the function down the road. 

