{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tools import glove_helper\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from itertools import groupby\n",
    "from os.path import basename, splitext\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the script, you will need to CMD and authenticate with \n",
    "\n",
    "'gcloud auth application-default login'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(project='manifest-frame-203601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "    \"\"\"\n",
    "    select distinct repo_path,c_content from w266_final.final_20k\n",
    "    \"\"\")\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "df = []\n",
    "for row in rows:\n",
    "    df.append([row.repo_path,row.c_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172413"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.columns = ['repo_path','content']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(docstring_list):\n",
    "    \n",
    "    \"\"\"takes a list of doc strings and converts to a single flat list of tokens\"\"\"\n",
    "    \n",
    "    tokens = [tf.keras.preprocessing.text.text_to_word_sequence(i) for i in docstring_list]\n",
    "    flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "    flat_string = \" \".join(flat_tokens)\n",
    "    \n",
    "    return flat_string\n",
    "\n",
    "def get_docstrings(source):\n",
    "    \n",
    "    \"\"\"function to walk through parse tree and return list of docstrings\"\"\"\n",
    "    \n",
    "    NODE_TYPES = {\n",
    "    ast.ClassDef: 'Class',\n",
    "    ast.FunctionDef: 'Function/Method',\n",
    "    ast.Module: 'Module'\n",
    "    }\n",
    "    \n",
    "    docstrings = []\n",
    "    \n",
    "    try:\n",
    "        tree = ast.parse(source)\n",
    "    except:\n",
    "        return \" \"\n",
    "       \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, tuple(NODE_TYPES)):\n",
    "            docstring = ast.get_docstring(node)\n",
    "            docstrings.append(docstring)\n",
    "    \n",
    "    docstrings =  [x for x in docstrings if x is not None]\n",
    "    clean_string = cleanup(docstrings)\n",
    "            \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docstrings'] = [get_docstrings(x) for x in list(df['content'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "hands = glove_helper.Hands(ndim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up corpus for count vectorizer\n",
    "corpus = list(df['docstrings'])\n",
    "\n",
    "#count values for tfidf calculations\n",
    "count_vect = CountVectorizer()\n",
    "count_vect = count_vect.fit(corpus)\n",
    "freq_term_matrix = count_vect.transform(corpus)\n",
    "\n",
    "#to grab columns for words\n",
    "vocab = count_vect.vocabulary_\n",
    "\n",
    "#create a holder for the new df column\n",
    "embeddings_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_embed(words):\n",
    "    \n",
    "    global count_vect, freq_term_matrix, vocab\n",
    "    \n",
    "    #verify there are docstrings available\n",
    "    if len(words)==0:\n",
    "        return np.zeros(100)\n",
    "         \n",
    "    #create tfidf for each document\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    tfidf.fit(freq_term_matrix)\n",
    "    doc_freq_term = count_vect.transform([words])\n",
    "    idfs = tfidf.transform(doc_freq_term)\n",
    "\n",
    "    #split the docstrings to individual words for average\n",
    "    sent_list = words.split(\" \")\n",
    "    embeddings = []\n",
    "\n",
    "    #cycle through list of words in docstring\n",
    "    for i in range(len(sent_list)):\n",
    "\n",
    "        if sent_list[i] in vocab:\n",
    "\n",
    "            col = vocab[sent_list[i]]\n",
    "            embed = hands.get_vector(sent_list[i], strict=False)\n",
    "            tfidf = idfs[0, col]\n",
    "            embeddings.append(np.multiply(embed, tfidf))\n",
    "\n",
    "        embed_array = np.asarray(embeddings)\n",
    "        \n",
    "        if len(embed_array)==0:\n",
    "            return np.zeros(100)\n",
    "\n",
    "        return np.mean(embed_array, axis=0)\n",
    "    \n",
    "def find_nn(words, embeddings):\n",
    "    \n",
    "    search = words_to_embed(words)\n",
    "    distances = [scipy.spatial.distance.cosine(search, i) for i in embeddings]\n",
    "    nn = np.argsort(np.asarray(distances))\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005800026680122729\n",
      "0.011600053360245458\n",
      "0.017400080040368186\n",
      "0.023200106720490915\n",
      "0.029000133400613644\n",
      "0.03480016008073637\n",
      "0.0406001867608591\n",
      "0.04640021344098183\n",
      "0.05220024012110456\n",
      "0.05800026680122729\n",
      "0.06380029348135001\n",
      "0.06960032016147275\n",
      "0.07540034684159547\n",
      "0.0812003735217182\n",
      "0.08700040020184092\n",
      "0.09280042688196366\n",
      "0.09860045356208638\n",
      "0.10440048024220912\n",
      "0.11020050692233184\n",
      "0.11600053360245458\n",
      "0.1218005602825773\n",
      "0.12760058696270002\n",
      "0.13340061364282277\n",
      "0.1392006403229455\n",
      "0.1450006670030682\n",
      "0.15080069368319093\n",
      "0.15660072036331368\n",
      "0.1624007470434364\n",
      "0.16820077372355913\n",
      "0.17400080040368185\n",
      "0.1798008270838046\n",
      "0.18560085376392732\n",
      "0.19140088044405004\n",
      "0.19720090712417276\n",
      "0.2030009338042955\n",
      "0.20880096048441824\n",
      "0.21460098716454096\n",
      "0.22040101384466368\n",
      "0.2262010405247864\n",
      "0.23200106720490915\n",
      "0.23780109388503187\n",
      "0.2436011205651546\n",
      "0.24940114724527732\n",
      "0.25520117392540004\n",
      "0.2610012006055228\n",
      "0.26680122728564554\n",
      "0.27260125396576823\n",
      "0.278401280645891\n",
      "0.2842013073260137\n",
      "0.2900013340061364\n",
      "0.2958013606862592\n",
      "0.30160138736638187\n",
      "0.3074014140465046\n",
      "0.31320144072662737\n",
      "0.31900146740675006\n",
      "0.3248014940868728\n",
      "0.3306015207669955\n",
      "0.33640154744711825\n",
      "0.342201574127241\n",
      "0.3480016008073637\n",
      "0.35380162748748645\n",
      "0.3596016541676092\n",
      "0.3654016808477319\n",
      "0.37120170752785464\n",
      "0.37700173420797733\n",
      "0.3828017608881001\n",
      "0.38860178756822283\n",
      "0.39440181424834553\n",
      "0.4002018409284683\n",
      "0.406001867608591\n",
      "0.4118018942887137\n",
      "0.41760192096883647\n",
      "0.42340194764895916\n",
      "0.4292019743290819\n",
      "0.43500200100920466\n",
      "0.44080202768932736\n",
      "0.4466020543694501\n",
      "0.4524020810495728\n",
      "0.45820210772969555\n",
      "0.4640021344098183\n",
      "0.469802161089941\n",
      "0.47560218777006374\n",
      "0.4814022144501865\n",
      "0.4872022411303092\n",
      "0.49300226781043194\n",
      "0.49880229449055463\n",
      "0.5046023211706774\n",
      "0.5104023478508001\n",
      "0.5162023745309229\n",
      "0.5220024012110456\n",
      "0.5278024278911683\n",
      "0.5336024545712911\n",
      "0.5394024812514138\n",
      "0.5452025079315365\n",
      "0.5510025346116593\n",
      "0.556802561291782\n",
      "0.5626025879719047\n",
      "0.5684026146520273\n",
      "0.5742026413321502\n",
      "0.5800026680122728\n",
      "0.5858026946923955\n",
      "0.5916027213725183\n",
      "0.597402748052641\n",
      "0.6032027747327637\n",
      "0.6090028014128865\n",
      "0.6148028280930092\n",
      "0.6206028547731319\n",
      "0.6264028814532547\n",
      "0.6322029081333774\n",
      "0.6380029348135001\n",
      "0.6438029614936228\n",
      "0.6496029881737456\n",
      "0.6554030148538683\n",
      "0.661203041533991\n",
      "0.6670030682141138\n",
      "0.6728030948942365\n",
      "0.6786031215743592\n",
      "0.684403148254482\n",
      "0.6902031749346047\n",
      "0.6960032016147274\n",
      "0.7018032282948502\n",
      "0.7076032549749729\n",
      "0.7134032816550956\n",
      "0.7192033083352184\n",
      "0.7250033350153411\n",
      "0.7308033616954638\n",
      "0.7366033883755865\n",
      "0.7424034150557093\n",
      "0.748203441735832\n",
      "0.7540034684159547\n",
      "0.7598034950960775\n",
      "0.7656035217762002\n",
      "0.7714035484563229\n",
      "0.7772035751364457\n",
      "0.7830036018165684\n",
      "0.7888036284966911\n",
      "0.7946036551768139\n",
      "0.8004036818569366\n",
      "0.8062037085370592\n",
      "0.812003735217182\n",
      "0.8178037618973047\n",
      "0.8236037885774274\n",
      "0.8294038152575501\n",
      "0.8352038419376729\n",
      "0.8410038686177956\n",
      "0.8468038952979183\n",
      "0.8526039219780411\n",
      "0.8584039486581638\n",
      "0.8642039753382865\n",
      "0.8700040020184093\n",
      "0.875804028698532\n",
      "0.8816040553786547\n",
      "0.8874040820587775\n",
      "0.8932041087389002\n",
      "0.8990041354190229\n",
      "0.9048041620991456\n",
      "0.9106041887792684\n",
      "0.9164042154593911\n",
      "0.9222042421395138\n",
      "0.9280042688196366\n",
      "0.9338042954997593\n",
      "0.939604322179882\n",
      "0.9454043488600048\n",
      "0.9512043755401275\n",
      "0.9570044022202502\n",
      "0.962804428900373\n",
      "0.9686044555804957\n",
      "0.9744044822606184\n",
      "0.9802045089407412\n",
      "0.9860045356208639\n",
      "0.9918045623009866\n",
      "0.9976045889811093\n"
     ]
    }
   ],
   "source": [
    "#df['embeddings'] = [words_to_embed(x) for x in list(df['docstrings'])]\n",
    "\n",
    "embeddings = []\n",
    "i=0\n",
    "for x in list(df['docstrings']):\n",
    "    i+=1\n",
    "    embeddings.append(words_to_embed(x))\n",
    "    if i%1000==0:\n",
    "        print(i/172413)\n",
    "        \n",
    "df['embeddings'] = embeddings\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_path</th>\n",
       "      <th>content</th>\n",
       "      <th>docstrings</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beavyHQ/beavy beavy_modules/private_messaging/...</td>\n",
       "      <td>\"\"\"empty message\\n\\nRevision ID: 47de1903b00\\n...</td>\n",
       "      <td>empty message revision id 47de1903b00 revises ...</td>\n",
       "      <td>[-0.05100566, 0.11278218, 0.043621335, -0.0586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antergos/Cnchi src/pages/dialogs/partition_bas...</td>\n",
       "      <td>#!/usr/bin/env python\\n# -*- coding: utf-8 -*-...</td>\n",
       "      <td>base class for create edit partition dialogs c...</td>\n",
       "      <td>[-0.2872601, 0.15455353, 0.062218327, 0.038352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wolkstein/OpenDroneMap-GCP_LIST.TXT-generator ...</td>\n",
       "      <td>import numpy as np\\nimport cv2\\nimport argpars...</td>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dstndstn/astrometry.net doc/UCAC3_guide/build-...</td>\n",
       "      <td>#Script for automatically running astrometry.n...</td>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ClusterLabs/booth test/assertions.py</td>\n",
       "      <td>import re\\n\\nclass BoothAssertions:\\n    def c...</td>\n",
       "      <td>fail the test unless the text matches the regu...</td>\n",
       "      <td>[-0.11991147, 0.037409756, 0.09766561, -0.0026...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           repo_path  \\\n",
       "0  beavyHQ/beavy beavy_modules/private_messaging/...   \n",
       "1  Antergos/Cnchi src/pages/dialogs/partition_bas...   \n",
       "2  wolkstein/OpenDroneMap-GCP_LIST.TXT-generator ...   \n",
       "3  dstndstn/astrometry.net doc/UCAC3_guide/build-...   \n",
       "4               ClusterLabs/booth test/assertions.py   \n",
       "\n",
       "                                             content  \\\n",
       "0  \"\"\"empty message\\n\\nRevision ID: 47de1903b00\\n...   \n",
       "1  #!/usr/bin/env python\\n# -*- coding: utf-8 -*-...   \n",
       "2  import numpy as np\\nimport cv2\\nimport argpars...   \n",
       "3  #Script for automatically running astrometry.n...   \n",
       "4  import re\\n\\nclass BoothAssertions:\\n    def c...   \n",
       "\n",
       "                                          docstrings  \\\n",
       "0  empty message revision id 47de1903b00 revises ...   \n",
       "1  base class for create edit partition dialogs c...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  fail the test unless the text matches the regu...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.05100566, 0.11278218, 0.043621335, -0.0586...  \n",
       "1  [-0.2872601, 0.15455353, 0.062218327, 0.038352...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [-0.11991147, 0.037409756, 0.09766561, -0.0026...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_code(search_terms, docstrings, embeddings, n):\n",
    "    \n",
    "    top_n = find_nn(search_terms, embeddings)[0:n]\n",
    "    code = [df['content'][i] for i in top_n]\n",
    "    \n",
    "    return code\n",
    "\n",
    "doc_strings = list(df['docstrings'])\n",
    "embed_vecs = list(df['embeddings'])\n",
    "\n",
    "def make_query_file(query, results, filename):\n",
    "    \n",
    "    output = open(filename, 'w')\n",
    "    for item in results:\n",
    "        output.write(\"Query: \"+query+'\\n')\n",
    "        output.write(\"\\n************************** NEXT RESULT **************************************\\n\")\n",
    "        output.write(\"%s\\n\" % item)\n",
    "        \n",
    "    return \n",
    "\n",
    "def keyword_search(word, corpus, content):\n",
    "    \n",
    "    word = word.lower()\n",
    "    query_results = []\n",
    "    for i in range(len(corpus)):\n",
    "        if word in corpus[i]:\n",
    "            query_results.append(content[i])\n",
    "            \n",
    "    return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = \"function that calculates distance\"\n",
    "search2 = 'merge two lists'\n",
    "search3 = 'remove duplicates from sorted array'\n",
    "search3 = 'determine if a Sudoku is valid'\n",
    "search4 = 'unique binary search tree'\n",
    "search5 = 'voice recognition function'\n",
    "search6 = 'LSTM model for semantic search'\n",
    "\n",
    "searches = [search1, search2, search3, search4, search5, search6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(searches)):\n",
    "    query = top_n_code(searches[i], doc_strings, embed_vecs, 10)\n",
    "    x=i+1\n",
    "    filename = 'model_1_queries/query'+str(x)+'.txt'\n",
    "    make_query_file(searches[i], query, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Chapter 6: Arrays\n",
      "\"\"\"\n",
      "import random\n",
      "import sys\n",
      "from datetime import datetime\n",
      "\n",
      "import attr\n",
      "from attr.validators import instance_of\n",
      "\n",
      "\n",
      "def swap(arr, idx, idy):\n",
      "    tmp = arr[idx]\n",
      "    arr[idx] = arr[idy]\n",
      "    arr[idy] = tmp\n",
      "\n",
      "\n",
      "def dutch_national_partition(idx, arr):\n",
      "    \"\"\"\n",
      "    Problem 6.1\n",
      "    Partition such that all elements less than\n",
      "    arr[idx] come first, then all elements equal\n",
      "    to arr[idx], then all elements greater than\n",
      "    arr[idx]\n",
      "    \"\"\"\n",
      "    pivot = arr[idx]\n",
      "\n",
      "    smaller = 0\n",
      "    idy = smaller\n",
      "    while idy < len(arr):\n",
      "        # group all elements less than pivot at\n",
      "        # the bottom\n",
      "        if arr[idy] < pivot:\n",
      "            swap(arr, smaller, idy)\n",
      "            smaller += 1\n",
      "        idy += 1\n",
      "\n",
      "    larger = len(arr) - 1\n",
      "    idy = larger\n",
      "    while idy >= 0:\n",
      "        # group all elements greater than pivot\n",
      "        # at the bottom\n",
      "        if arr[idy] > pivot:\n",
      "            swap(arr, larger, idy)\n",
      "            larger -= 1\n",
      "        idy -= 1\n",
      "\n",
      "\n",
      "def dutch_partition_better(idx, arr):\n",
      "    \"\"\"\n",
      "    improved, single-pass version of\n",
      "    dutch national flag algorithm\n",
      "    \"\"\"\n",
      "    pivot = arr[idx]\n",
      "\n",
      "    small = 0\n",
      "    equal = 0\n",
      "    large = len(arr) - 1\n",
      "\n",
      "    while equal < large:\n",
      "        if arr[equal] < pivot:\n",
      "            swap(arr, small, equal)\n",
      "            small += 1\n",
      "            equal += 1\n",
      "        elif arr[equal] == pivot:\n",
      "            equal += 1\n",
      "        else:\n",
      "            swap(arr, equal, large)\n",
      "            large -= 1\n",
      "\n",
      "\n",
      "def add_one(num):\n",
      "    \"\"\"\n",
      "    Question 6.2: Implement arbitrary precision\n",
      "    integer to add one\n",
      "    \"\"\"\n",
      "    num[-1] += 1\n",
      "    begin = len(num) - 1\n",
      "\n",
      "    while num[begin] == 10 and begin > 0:\n",
      "        num[begin] = 0\n",
      "        num[begin - 1] += 1\n",
      "        begin -= 1\n",
      "\n",
      "    if num[0] == 10:\n",
      "        num[0] = 0\n",
      "        return [1] + num\n",
      "    return num\n",
      "\n",
      "\n",
      "def multiply(first_num, second_num):\n",
      "    \"\"\"\n",
      "    Question 6.3: Multiply two arbitrary\n",
      "    precision numbers\n",
      "    \"\"\"\n",
      "    first_num.reverse()\n",
      "    second_num.reverse()\n",
      "\n",
      "    result_len = len(first_num) + len(second_num)\n",
      "    result = [0 for _ in xrange(result_len)]\n",
      "    negative = False\n",
      "\n",
      "    for idx_first, elt_first in enumerate(first_num):\n",
      "        for idx_second, elt_second in enumerate(second_num):\n",
      "            mult = elt_first * elt_second\n",
      "            if mult < 0:\n",
      "                negative = True\n",
      "                mult *= -1\n",
      "            result[idx_first + idx_second] += mult\n",
      "\n",
      "    carry = 0\n",
      "    for idx, elt in enumerate(result):\n",
      "        elt += carry\n",
      "        carry = elt // 10\n",
      "        result[idx] = elt % 10\n",
      "\n",
      "    while carry:\n",
      "        rem = carry % 10\n",
      "        carry //= 10\n",
      "        result.append(rem)\n",
      "\n",
      "    if negative:\n",
      "        result[-1] *= -1\n",
      "\n",
      "    result.reverse()\n",
      "\n",
      "    return result\n",
      "\n",
      "\n",
      "def can_reach_end(steps):\n",
      "    \"\"\"\n",
      "    Question 6.4: Given an array of n integers, where\n",
      "    A[i] denotes the maximum number of steps that can\n",
      "    advance from i, return whether it is possible to\n",
      "    advance to last index from beginning of array\n",
      "    \"\"\"\n",
      "    last_idx = len(steps)\n",
      "    furthest_reach_so_far = 0\n",
      "\n",
      "    idx = 0\n",
      "    while idx <= furthest_reach_so_far < last_idx:\n",
      "        furthest_reach_so_far = max(\n",
      "            furthest_reach_so_far,\n",
      "            steps[idx] + idx\n",
      "        )\n",
      "        idx += 1\n",
      "\n",
      "    return furthest_reach_so_far >= last_idx\n",
      "\n",
      "\n",
      "def delete_key(ls, key):\n",
      "    \"\"\"\n",
      "    Question 6.5: Given an array and an input key,\n",
      "    remove the key from the array\n",
      "    \"\"\"\n",
      "    write_idx = 0\n",
      "    for idx, elt in enumerate(ls):\n",
      "        if elt != key:\n",
      "            ls[write_idx] = elt\n",
      "            write_idx += 1\n",
      "\n",
      "    while write_idx < len(ls):\n",
      "        ls[write_idx] = None\n",
      "        write_idx += 1\n",
      "\n",
      "    return ls\n",
      "\n",
      "\n",
      "def delete_duplicates(ls):\n",
      "    \"\"\"\n",
      "    Question 6.6: Delete duplicates from sorted array,\n",
      "    return number of elements remainint\n",
      "    \"\"\"\n",
      "\n",
      "    if not len(ls):\n",
      "        return 0\n",
      "\n",
      "    write_idx = 0\n",
      "    for idx, elt in enumerate(ls):\n",
      "        if idx > write_idx and elt != ls[write_idx]:\n",
      "            write_idx += 1\n",
      "            ls[write_idx] = elt\n",
      "\n",
      "    return write_idx + 1\n",
      "\n",
      "\n",
      "def buy_sell_once(stocks):\n",
      "    \"\"\"\n",
      "    Problem 6.7\n",
      "    find the maximum profit achieved by buying\n",
      "    a stock and selling it\n",
      "    \"\"\"\n",
      "    max_profit = 0\n",
      "    lowest = sys.maxsize\n",
      "    for price in stocks:\n",
      "        if price < lowest:\n",
      "            lowest = price\n",
      "        if price - lowest > max_profit:\n",
      "            max_profit = price - lowest\n",
      "    return max_profit\n",
      "\n",
      "\n",
      "def buy_sell_stock_twice(stocks):\n",
      "    \"\"\"\n",
      "    Question 6.8: Find the maximum profit that\n",
      "    can be made from buying and selling a\n",
      "    stock at most twice\n",
      "    \"\"\"\n",
      "    max_profit = 0\n",
      "    for idx in xrange(len(stocks)):\n",
      "        profit = buy_sell_once(stocks[:idx]) + \\\n",
      "            buy_sell_once(stocks[idx:])\n",
      "        max_profit = max(profit, max_profit)\n",
      "    return max_profit\n",
      "\n",
      "\n",
      "def generate_primes(n):\n",
      "    \"\"\"\n",
      "    Question 6.9: Enumerate all primes to n\n",
      "    \"\"\"\n",
      "    primes = []\n",
      "    for idx in xrange(2, n):\n",
      "        divided = False\n",
      "        for prime in primes:\n",
      "            if idx % prime == 0:\n",
      "                divided = True\n",
      "                break\n",
      "        if not divided:\n",
      "            primes.append(idx)\n",
      "    return primes\n",
      "\n",
      "\n",
      "def apply_permutation(array, permutation):\n",
      "    \"\"\"\n",
      "    Question 6.10: Given an array and a permutation,\n",
      "    apply the permutation to the array\n",
      "    \"\"\"\n",
      "    perm_size = len(permutation)\n",
      "    for idx in xrange(perm_size):\n",
      "        next = idx\n",
      "        while permutation[next] >= 0:\n",
      "            array[idx], array[permutation[next]] = \\\n",
      "                array[permutation[next]], array[idx]\n",
      "            temp = permutation[next]\n",
      "            permutation[next] -= perm_size\n",
      "            next = temp\n",
      "\n",
      "    return array\n",
      "\n",
      "\n",
      "def next_permutation(ls):\n",
      "    \"\"\"\n",
      "    Question 6.11: Given an array, compute the\n",
      "    next permutation in dictionary order\n",
      "    \"\"\"\n",
      "    # find right-most adjacent indices with elements\n",
      "    # in order\n",
      "    inorder_idx = None\n",
      "    cur_cand = sys.maxint\n",
      "    cand_idx = None\n",
      "    for idx, elt in enumerate(ls[:-1]):\n",
      "        if elt < ls[idx + 1]:\n",
      "            inorder_idx = idx\n",
      "        if inorder_idx is not None:\n",
      "            if ls[inorder_idx] < elt < cur_cand:\n",
      "                cur_cand = elt\n",
      "                cand_idx = idx\n",
      "\n",
      "    if inorder_idx is not None:\n",
      "        if ls[inorder_idx] < ls[-1] < cur_cand:\n",
      "            cand_idx = len(ls) - 1\n",
      "\n",
      "    if inorder_idx is None:\n",
      "        return []\n",
      "\n",
      "    # switch candidate element with first element that is in order\n",
      "    ls[inorder_idx], ls[cand_idx] = ls[cand_idx], ls[inorder_idx]\n",
      "    return ls[:inorder_idx + 1] + ls[inorder_idx + 1:][::-1]\n",
      "\n",
      "\n",
      "def random_sample(inputs, size):\n",
      "    \"\"\"\n",
      "    Problem 6.12\n",
      "    given an array of input values, return\n",
      "    a random subset of inputs of size `size`\n",
      "    \"\"\"\n",
      "    next_pos = len(inputs) - 1\n",
      "    # seed rand generator\n",
      "    random.seed(datetime.now())\n",
      "    for _ in xrange(size):\n",
      "        if next_pos > 0:\n",
      "            idx = random.randrange(next_pos + 1)\n",
      "            swap(inputs, idx, next_pos)\n",
      "            next_pos -= 1\n",
      "\n",
      "    return inputs[next_pos:]\n",
      "\n",
      "\n",
      "def online_sample(population, size):\n",
      "    \"\"\"\n",
      "    Problem 6.13: Given a stream of packets and\n",
      "    an desired target size k, maintain a uniform\n",
      "    random subset of size k\n",
      "    \"\"\"\n",
      "    results = []\n",
      "    for idx, elt in enumerate(population):\n",
      "        if idx < size:\n",
      "            results.append(idx)\n",
      "        else:\n",
      "            probability = float(idx + 1) / float(size)\n",
      "            if random.uniform(0, 1) < probability:\n",
      "                # add number to set\n",
      "                results[random.randrange(0, size)] = elt\n",
      "    return results\n",
      "\n",
      "\n",
      "def random_permutation(n):\n",
      "    \"\"\"\n",
      "    Question 6.14: Compute a random permutation of\n",
      "    numbers from 0 to n - 1\n",
      "    \"\"\"\n",
      "    ls = range(n)\n",
      "    return random_sample(ls, n)\n",
      "\n",
      "\n",
      "def random_subset(n, k):\n",
      "    \"\"\"\n",
      "    Question 6.15: Given a number n, compute\n",
      "    a random subset of 0 to n - 1 of size k\n",
      "    \"\"\"\n",
      "    seen = {}\n",
      "\n",
      "    for idx in xrange(k):\n",
      "        rand_idx = random.randrange(n)\n",
      "        rand_val = seen.get(rand_idx, None)\n",
      "        idx_val = seen.get(idx, None)\n",
      "\n",
      "        if rand_val is None and idx_val is None:\n",
      "            seen[rand_idx] = idx\n",
      "            seen[idx] = rand_idx\n",
      "        elif rand_val is None and idx_val is not None:\n",
      "            seen[rand_idx] = idx_val\n",
      "            seen[idx] = rand_idx\n",
      "        elif rand_val is not None and idx_val is None:\n",
      "            seen[rand_idx] = idx\n",
      "            seen[idx] = rand_val\n",
      "        else:\n",
      "            tmp = idx_val\n",
      "            seen[idx] = rand_val\n",
      "            seen[rand_idx] = tmp\n",
      "\n",
      "    results = []\n",
      "    for idx in xrange(k):\n",
      "        results.append(seen[idx])\n",
      "\n",
      "    return results\n",
      "\n",
      "\n",
      "@attr.s\n",
      "class NumProbability(object):\n",
      "    num = attr.ib(validator=instance_of(int))\n",
      "    prob = attr.ib(validator=instance_of(float))\n",
      "\n",
      "\n",
      "def nonuniform_random(nums):\n",
      "    \"\"\"\n",
      "    Question 6.16: Generate nonuniform random numbers,\n",
      "    given array of numbers and probabilities\n",
      "    \"\"\"\n",
      "    endpts = [(0, None)]\n",
      "\n",
      "    for num in nums:\n",
      "        new_end = endpts[-1][0] + num.prob\n",
      "        endpts.append((new_end, num.num))\n",
      "\n",
      "    prob = random.uniform(0, 1)\n",
      "\n",
      "    for idx, elt in enumerate(endpts[:-1]):\n",
      "        if elt[0] < prob < endpts[idx + 1][0]:\n",
      "            return endpts[idx + 1][1]\n",
      "    return None\n",
      "\n",
      "\n",
      "def check_sudoku(sudoku):\n",
      "    \"\"\"\n",
      "    Question 6.17: Check if 9x9 sudoku puzzle is valid\n",
      "    \"\"\"\n",
      "    # initialize sets\n",
      "    rows = []\n",
      "    cols = []\n",
      "    squares = []\n",
      "    for idx in xrange(9):\n",
      "        rows.append(set())\n",
      "        cols.append(set())\n",
      "        squares.append(set())\n",
      "\n",
      "    for row, row_elt in enumerate(sudoku):\n",
      "        for col, elt in enumerate(row_elt):\n",
      "            if elt in rows[row]:\n",
      "                return False\n",
      "            rows[row].add(elt)\n",
      "\n",
      "            if elt in cols[col]:\n",
      "                return False\n",
      "            cols[col].add(elt)\n",
      "\n",
      "            square = get_square_idx(row, col)\n",
      "            if elt in squares[square]:\n",
      "                return False\n",
      "            squares[square].add(elt)\n",
      "    return True\n",
      "\n",
      "\n",
      "def get_square_idx(row, col):\n",
      "    \"\"\"\n",
      "    Get index of associated square\n",
      "    \"\"\"\n",
      "    return (row // 3) * 3 + col // 3\n",
      "\n",
      "\n",
      "def spiralize(arr):\n",
      "    \"\"\"\n",
      "    Problem 6.18\n",
      "    print out spiral traversal of 2D array\n",
      "    \"\"\"\n",
      "    output = []\n",
      "    min_x = 0\n",
      "    min_y = 0\n",
      "    max_x = len(arr[0]) - 1\n",
      "    max_y = len(arr) - 1\n",
      "\n",
      "    while min_x <= max_x and min_y <= max_y:\n",
      "        # iterate forward across top\n",
      "        for x in xrange(min_x, max_x + 1):\n",
      "            output.append(arr[min_y][x])\n",
      "        min_y += 1\n",
      "\n",
      "        # iterate down across right\n",
      "        for y in xrange(min_y, max_y + 1):\n",
      "            output.append(arr[y][max_x])\n",
      "        max_x -= 1\n",
      "\n",
      "        # iterate backward across bottom\n",
      "        for x in xrange(max_x, min_x - 1, -1):\n",
      "            output.append(arr[max_y][x])\n",
      "        max_y -= 1\n",
      "\n",
      "        # iterate upward across left\n",
      "        for y in xrange(max_y, min_y - 1, -1):\n",
      "            output.append(arr[y][min_x])\n",
      "        min_x += 1\n",
      "    return output\n",
      "\n",
      "\n",
      "def rotate_matrix(matrix):\n",
      "    \"\"\"\n",
      "    Question 6.19: Rotate a 2d array\n",
      "    \"\"\"\n",
      "    matrix_size = len(matrix) - 1\n",
      "    for i in xrange(len(matrix) / 2):\n",
      "        for j in xrange(i, matrix_size - i):\n",
      "            temp1 = matrix[matrix_size - j][i]\n",
      "            temp2 = matrix[matrix_size - i][matrix_size - j]\n",
      "            temp3 = matrix[j][matrix_size - i]\n",
      "            temp4 = matrix[i][j]\n",
      "            matrix[i][j] = temp1\n",
      "            matrix[matrix_size - j][i] = temp2\n",
      "            matrix[matrix_size - i][matrix_size - j] = temp3\n",
      "            matrix[j][matrix_size - i] = temp4\n",
      "    return matrix\n",
      "\n",
      "\n",
      "def generate_pascal_triangle(num_rows):\n",
      "    \"\"\"\n",
      "    Question 6.20: Generate the first `num_rows` of\n",
      "    a Pascal Triangle\n",
      "    \"\"\"\n",
      "    result = []\n",
      "    last_row = []\n",
      "\n",
      "    while num_rows:\n",
      "        cur = [1]\n",
      "\n",
      "        # if we don't have an empty array\n",
      "        if last_row:\n",
      "            for idx, elt in enumerate(last_row[:-1]):\n",
      "                cur.append(elt + last_row[idx + 1])\n",
      "            cur.append(1)\n",
      "        result.append(cur)\n",
      "        last_row = cur\n",
      "\n",
      "        num_rows -= 1\n",
      "\n",
      "    return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Search 3 Backcheck\n",
    "content = df['content']\n",
    "q3_check = keyword_search('sudoku', doc_strings, content)\n",
    "print(q3_check[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy\n",
      "import six\n",
      "\n",
      "import chainer\n",
      "from chainer.backends import cuda\n",
      "from chainer.backends import intel64\n",
      "from chainer import function\n",
      "from chainer import function_node\n",
      "from chainer.utils import type_check\n",
      "\n",
      "\n",
      "def _extract_gates(x):\n",
      "    r = x.reshape((len(x), x.shape[1] // 4, 4) + x.shape[2:])\n",
      "    return [r[:, :, i] for i in six.moves.range(4)]\n",
      "\n",
      "\n",
      "def _sigmoid(x, xp=numpy):\n",
      "    half = x.dtype.type(0.5)\n",
      "    return xp.tanh(x * half) * half + half\n",
      "\n",
      "\n",
      "def _grad_sigmoid(x):\n",
      "    return x * (1 - x)\n",
      "\n",
      "\n",
      "def _grad_grad_sigmoid(x):\n",
      "    return x * (1 - x) * (1 - 2 * x)\n",
      "\n",
      "\n",
      "def _grad_tanh(x):\n",
      "    return 1 - x * x\n",
      "\n",
      "\n",
      "def _grad_grad_tanh(x, gx):\n",
      "    return -2 * x * gx\n",
      "\n",
      "\n",
      "_preamble = '''\n",
      "template <typename T> __device__ T sigmoid(T x) {\n",
      "    const T half = 0.5;\n",
      "    return tanh(x * half) * half + half;\n",
      "}\n",
      "template <typename T> __device__ T grad_sigmoid(T y) { return y * (1 - y); }\n",
      "template <typename T> __device__ T grad_tanh(T y) { return 1 - y * y; }\n",
      "\n",
      "#define COMMON_ROUTINE \\\n",
      "    T aa = tanh(a); \\\n",
      "    T ai = sigmoid(i_); \\\n",
      "    T af = sigmoid(f); \\\n",
      "    T ao = sigmoid(o);\n",
      "'''\n",
      "\n",
      "\n",
      "class LSTM(function_node.FunctionNode):\n",
      "\n",
      "    \"\"\"Long short-term memory unit with forget gate.\n",
      "\n",
      "    It has two inputs (c, x) and two outputs (c, h), where c indicates the cell\n",
      "    state. x must have four times channels compared to the number of units.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def check_type_forward(self, in_types):\n",
      "        type_check.expect(in_types.size() == 2)\n",
      "        c_type, x_type = in_types\n",
      "\n",
      "        type_check.expect(\n",
      "            c_type.dtype.kind == 'f',\n",
      "            x_type.dtype == c_type.dtype,\n",
      "\n",
      "            c_type.ndim >= 2,\n",
      "            x_type.ndim >= 2,\n",
      "            c_type.ndim == x_type.ndim,\n",
      "\n",
      "            x_type.shape[0] <= c_type.shape[0],\n",
      "            x_type.shape[1] == 4 * c_type.shape[1],\n",
      "        )\n",
      "        for i in six.moves.range(2, type_check.eval(c_type.ndim)):\n",
      "            type_check.expect(x_type.shape[i] == c_type.shape[i])\n",
      "\n",
      "    def forward(self, inputs):\n",
      "        self.retain_inputs((0, 1))\n",
      "        c_prev, x = inputs\n",
      "        a, i, f, o = _extract_gates(x)\n",
      "        batch = len(x)\n",
      "\n",
      "        if isinstance(x, chainer.get_cpu_array_types()):\n",
      "            if intel64.should_use_ideep('>=auto'):\n",
      "                xp = intel64.ideep.get_array_module(x)\n",
      "            else:\n",
      "                xp = numpy\n",
      "            a = xp.tanh(a)\n",
      "            i = _sigmoid(i, xp)\n",
      "            f = _sigmoid(f, xp)\n",
      "            o = _sigmoid(o, xp)\n",
      "\n",
      "            c_next = numpy.empty_like(c_prev)\n",
      "            c_next[:batch] = a * i + f * c_prev[:batch]\n",
      "            h = o * xp.tanh(c_next[:batch])\n",
      "        else:\n",
      "            c_next = cuda.cupy.empty_like(c_prev)\n",
      "            h = cuda.cupy.empty_like(c_next[:batch])\n",
      "            cuda.elementwise(\n",
      "                'T c_prev, T a, T i_, T f, T o', 'T c, T h',\n",
      "                '''\n",
      "                    COMMON_ROUTINE;\n",
      "                    c = aa * ai + af * c_prev;\n",
      "                    h = ao * tanh(c);\n",
      "                ''',\n",
      "                'lstm_fwd', preamble=_preamble)(\n",
      "                    c_prev[:batch], a, i, f, o, c_next[:batch], h)\n",
      "\n",
      "        c_next[batch:] = c_prev[batch:]\n",
      "        self.retain_outputs((0,))\n",
      "        return c_next, h\n",
      "\n",
      "    def backward(self, indexes, grads):\n",
      "        grad_inputs = (\n",
      "            self.get_retained_inputs() + self.get_retained_outputs() + grads)\n",
      "        return LSTMGrad()(*grad_inputs)\n",
      "\n",
      "\n",
      "class LSTMGrad(function.Function):\n",
      "\n",
      "    def forward(self, inputs):\n",
      "        xp = cuda.get_array_module(*inputs)\n",
      "        c_prev, x, c_next, gc, gh = inputs\n",
      "        batch = len(x)\n",
      "\n",
      "        gx = xp.empty_like(x)\n",
      "        ga, gi, gf, go = _extract_gates(gx)\n",
      "\n",
      "        # Consider the case that either gradient is not given\n",
      "        if gc is None:\n",
      "            gc_update = 0\n",
      "            gc_rest = 0\n",
      "        else:\n",
      "            gc_update = gc[:batch]\n",
      "            gc_rest = gc[batch:]\n",
      "        if gh is None:\n",
      "            gh = 0\n",
      "\n",
      "        a, i, f, o = _extract_gates(x)\n",
      "        if xp is numpy:\n",
      "            if intel64.should_use_ideep('>=auto'):\n",
      "                xp = intel64.ideep.get_array_module(x)\n",
      "            tanh_a = xp.tanh(a)\n",
      "            sig_i = _sigmoid(i, xp)\n",
      "            sig_f = _sigmoid(f, xp)\n",
      "            sig_o = _sigmoid(o, xp)\n",
      "\n",
      "            co = xp.tanh(c_next[:batch])\n",
      "            gc_prev = numpy.empty_like(c_prev)\n",
      "            # multiply f later\n",
      "            gc_prev[:batch] = gh * sig_o * _grad_tanh(co) + gc_update\n",
      "            gc = gc_prev[:batch]\n",
      "            ga[:] = gc * sig_i * _grad_tanh(tanh_a)\n",
      "            gi[:] = gc * tanh_a * _grad_sigmoid(sig_i)\n",
      "            gf[:] = gc * c_prev[:batch] * _grad_sigmoid(sig_f)\n",
      "            go[:] = gh * co * _grad_sigmoid(sig_o)\n",
      "            gc_prev[:batch] *= sig_f  # multiply f here\n",
      "            gc_prev[batch:] = gc_rest\n",
      "        else:\n",
      "            gc_prev = xp.empty_like(c_prev)\n",
      "            cuda.elementwise(\n",
      "                'T c_prev, T c, T gc, T gh, T a, T i_, T f, T o',\n",
      "                'T gc_prev, T ga, T gi, T gf, T go',\n",
      "                '''\n",
      "                    COMMON_ROUTINE;\n",
      "                    T co = tanh(c);\n",
      "                    T temp = gh * ao * grad_tanh(co) + gc;\n",
      "                    ga = temp * ai * grad_tanh(aa);\n",
      "                    gi = temp * aa * grad_sigmoid(ai);\n",
      "                    gf = temp * c_prev * grad_sigmoid(af);\n",
      "                    go = gh * co * grad_sigmoid(ao);\n",
      "                    gc_prev = temp * af;\n",
      "                ''',\n",
      "                'lstm_bwd', preamble=_preamble)(\n",
      "                    c_prev[:batch], c_next[:batch], gc_update, gh, a, i, f, o,\n",
      "                    gc_prev[:batch], ga, gi, gf, go)\n",
      "            gc_prev[batch:] = gc_rest\n",
      "\n",
      "        return gc_prev, gx\n",
      "\n",
      "    def backward(self, inputs, grads):\n",
      "        xp = cuda.get_array_module(*inputs)\n",
      "\n",
      "        c_prev, x, c, gc, gh = inputs\n",
      "        ggc_prev, ggx = grads\n",
      "        batch = len(x)\n",
      "\n",
      "        if gc is None:\n",
      "            gc = xp.zeros_like(c)\n",
      "        if gh is None:\n",
      "            gh = xp.zeros_like(c[:batch])\n",
      "        if ggc_prev is None:\n",
      "            ggc_prev = xp.zeros_like(c_prev)\n",
      "\n",
      "        gc_prev = xp.empty_like(c_prev)\n",
      "        gx = xp.empty_like(x)\n",
      "        gc_next = xp.empty_like(c)\n",
      "        ggc = xp.empty_like(ggc_prev)\n",
      "        ggh = xp.empty_like(gh)\n",
      "\n",
      "        gc_prev[batch:] = 0\n",
      "        gc_next[batch:] = 0\n",
      "        ggc[batch:] = ggc_prev[batch:]\n",
      "        ggh[batch:] = 0\n",
      "\n",
      "        c_prev = c_prev[:batch]\n",
      "        c = c[:batch]\n",
      "        gc = gc[:batch]\n",
      "        ggc_prev = ggc_prev[:batch]\n",
      "        ggx = ggx[:batch]\n",
      "\n",
      "        a, i, f, o = _extract_gates(x)\n",
      "        gga, ggi, ggf, ggo = _extract_gates(ggx)\n",
      "        ga, gi, gf, go = _extract_gates(gx)\n",
      "\n",
      "        lstm_grad_grad(\n",
      "            c_prev, a, i, f, o, c, gc, gh, ggc_prev, gga, ggi, ggf, ggo,\n",
      "            gc_prev[:batch], ga[:], gi[:], gf[:], go[:], gc_next[:batch],\n",
      "            ggc[:batch], ggh[:batch])\n",
      "        return gc_prev, gx, gc_next, ggc, ggh\n",
      "\n",
      "\n",
      "@cuda.fuse()\n",
      "def lstm_grad_grad(\n",
      "        c_prev, a, i, f, o, c, gc, gh, ggc_prev, gga, ggi, ggf, ggo,\n",
      "        gc_prev, ga, gi, gf, go, gc_next, ggc, ggh):\n",
      "    xp = cuda.get_array_module(a)\n",
      "    sig_o = _sigmoid(o, xp)\n",
      "    gsig_o = _grad_sigmoid(sig_o)\n",
      "    ggsig_o = _grad_grad_sigmoid(sig_o)\n",
      "    sig_i = _sigmoid(i, xp)\n",
      "    gsig_i = _grad_sigmoid(sig_i)\n",
      "    ggsig_i = _grad_grad_sigmoid(sig_i)\n",
      "    sig_f = _sigmoid(f, xp)\n",
      "    gsig_f = _grad_sigmoid(sig_f)\n",
      "    ggsig_f = _grad_grad_sigmoid(sig_f)\n",
      "    tanh_a = xp.tanh(a)\n",
      "    gtanh_a = _grad_tanh(tanh_a)\n",
      "    ggtanh_a = _grad_grad_tanh(tanh_a, gtanh_a)\n",
      "    tanh_c = xp.tanh(c)\n",
      "    gtanh_c = _grad_tanh(tanh_c)\n",
      "    ggtanh_c = _grad_grad_tanh(tanh_c, gtanh_c)\n",
      "\n",
      "    gc_bar = gh * sig_o * gtanh_c + gc\n",
      "\n",
      "    gc_prev[:] = ggf * gc_bar * gsig_f\n",
      "    ga[:] = (gga * sig_i * ggtanh_a + ggi * gtanh_a * gsig_i) * gc_bar\n",
      "    gi[:] = (gga * gtanh_a * gsig_i + ggi * tanh_a * ggsig_i) * gc_bar\n",
      "    gf[:] = (ggc_prev * (gh * sig_o * gtanh_c + gc) * gsig_f +\n",
      "             ggf * gc_bar * c_prev * ggsig_f)\n",
      "\n",
      "    ggc[:] = (ggc_prev * sig_f +\n",
      "              gga * sig_i * gtanh_a +\n",
      "              ggi * tanh_a * gsig_i +\n",
      "              ggf * c_prev * gsig_f)\n",
      "\n",
      "    dgc_do = gh * gsig_o * gtanh_c\n",
      "    go[:] = ggc * dgc_do + ggo * gh * tanh_c * ggsig_o\n",
      "    dgc_dc = gh * sig_o * ggtanh_c\n",
      "    gc_next[:] = ggc * dgc_dc + ggo * gh * gtanh_c * gsig_o\n",
      "    ggh[:] = ggc * sig_o * gtanh_c + ggo * tanh_c * gsig_o\n",
      "    return gc_prev, ga, gi, gf, go, gc_next, ggc, ggh\n",
      "\n",
      "\n",
      "def lstm(c_prev, x):\n",
      "    \"\"\"Long Short-Term Memory units as an activation function.\n",
      "\n",
      "    This function implements LSTM units with forget gates. Let the previous\n",
      "    cell state ``c_prev`` and the input array ``x``.\n",
      "\n",
      "    First, the input array ``x`` is split into four arrays\n",
      "    :math:`a, i, f, o` of the same shapes along the second axis. It means that\n",
      "    ``x`` 's second axis must have 4 times the ``c_prev`` 's second axis.\n",
      "\n",
      "    The split input arrays are corresponding to:\n",
      "\n",
      "        - :math:`a` : sources of cell input\n",
      "        - :math:`i` : sources of input gate\n",
      "        - :math:`f` : sources of forget gate\n",
      "        - :math:`o` : sources of output gate\n",
      "\n",
      "    Second, it computes the updated cell state ``c`` and the outgoing signal\n",
      "    ``h`` as:\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        c &= \\\\tanh(a) \\\\sigma(i)\n",
      "           + c_{\\\\text{prev}} \\\\sigma(f), \\\\\\\\\n",
      "        h &= \\\\tanh(c) \\\\sigma(o),\n",
      "\n",
      "    where :math:`\\\\sigma` is the elementwise sigmoid function.\n",
      "    These are returned as a tuple of two variables.\n",
      "\n",
      "    This function supports variable length inputs. The mini-batch size of\n",
      "    the current input must be equal to or smaller than that of the previous\n",
      "    one. When mini-batch size of ``x`` is smaller than that of ``c``, this\n",
      "    function only updates ``c[0:len(x)]`` and doesn't change the rest of ``c``,\n",
      "    ``c[len(x):]``.\n",
      "    So, please sort input sequences in descending order of lengths before\n",
      "    applying the function.\n",
      "\n",
      "    Args:\n",
      "        c_prev (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n",
      "        :class:`cupy.ndarray`):\n",
      "            Variable that holds the previous cell state. The cell state\n",
      "            should be a zero array or the output of the previous call of LSTM.\n",
      "        x (:class:`~chainer.Variable` or :class:`numpy.ndarray` or \\\n",
      "        :class:`cupy.ndarray`):\n",
      "            Variable that holds the sources of cell input, input gate, forget\n",
      "            gate and output gate. It must have the second dimension whose size\n",
      "            is four times of that of the cell state.\n",
      "\n",
      "    Returns:\n",
      "        tuple: Two :class:`~chainer.Variable` objects ``c`` and ``h``.\n",
      "        ``c`` is the updated cell state. ``h`` indicates the outgoing signal.\n",
      "\n",
      "    See the original paper proposing LSTM with forget gates:\n",
      "    `Long Short-Term Memory in Recurrent Neural Networks \\\n",
      "    <http://www.felixgers.de/papers/phd.pdf>`_.\n",
      "\n",
      "    .. seealso::\n",
      "        :class:`~chainer.links.LSTM`\n",
      "\n",
      "    .. admonition:: Example\n",
      "\n",
      "        Assuming ``y`` is the current incoming signal, ``c`` is the previous\n",
      "        cell state, and ``h`` is the previous outgoing signal from an ``lstm``\n",
      "        function. Each of ``y``, ``c`` and ``h`` has ``n_units`` channels.\n",
      "        Most typical preparation of ``x`` is:\n",
      "\n",
      "        >>> n_units = 100\n",
      "        >>> y = chainer.Variable(np.zeros((1, n_units), np.float32))\n",
      "        >>> h = chainer.Variable(np.zeros((1, n_units), np.float32))\n",
      "        >>> c = chainer.Variable(np.zeros((1, n_units), np.float32))\n",
      "        >>> model = chainer.Chain()\n",
      "        >>> with model.init_scope():\n",
      "        ...   model.w = L.Linear(n_units, 4 * n_units)\n",
      "        ...   model.v = L.Linear(n_units, 4 * n_units)\n",
      "        >>> x = model.w(y) + model.v(h)\n",
      "        >>> c, h = F.lstm(c, x)\n",
      "\n",
      "        It corresponds to calculate the input array ``x``, or the input\n",
      "        sources :math:`a, i, f, o`, from the current incoming signal ``y`` and\n",
      "        the previous outgoing signal ``h``. Different parameters are used for\n",
      "        different kind of input sources.\n",
      "\n",
      "    .. note::\n",
      "\n",
      "        We use the naming rule below.\n",
      "\n",
      "        - incoming signal\n",
      "            The formal input of the formulation of LSTM (e.g. in NLP, word\n",
      "            vector or output of lower RNN layer). The input of\n",
      "            :class:`chainer.links.LSTM` is the *incoming signal*.\n",
      "        - input array\n",
      "            The array which is linear transformed from *incoming signal* and\n",
      "            the previous outgoing signal. The *input array* contains four\n",
      "            sources, the sources of cell input, input gate, forget gate and\n",
      "            output gate. The input of\n",
      "            :class:`chainer.functions.activation.lstm.LSTM` is the\n",
      "            *input array*.\n",
      "\n",
      "    \"\"\"\n",
      "    return LSTM().apply((c_prev, x))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q6_check = keyword_search('LSTM', doc_strings, content)\n",
    "print(q6_check[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "\"\"\"\n",
      "Trains a neural network for singing voice detection.\n",
      "\n",
      "For usage information, call with --help.\n",
      "\n",
      "Author: Jan Schlter\n",
      "\"\"\"\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import io\n",
      "from argparse import ArgumentParser\n",
      "\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "floatX = theano.config.floatX\n",
      "import lasagne\n",
      "\n",
      "from progress import progress\n",
      "from simplecache import cached\n",
      "import audio\n",
      "import znorm\n",
      "from labels import create_aligned_targets\n",
      "import model\n",
      "import augment\n",
      "\n",
      "def opts_parser():\n",
      "    descr = \"Trains a neural network for singing voice detection.\"\n",
      "    parser = ArgumentParser(description=descr)\n",
      "    parser.add_argument('modelfile', metavar='MODELFILE',\n",
      "            type=str,\n",
      "            help='File to save the learned weights to (.npz format)')\n",
      "    parser.add_argument('--dataset',\n",
      "            type=str, default='jamendo',\n",
      "            help='Name of the dataset to use (default: %(default)s)')\n",
      "    parser.add_argument('--augment',\n",
      "            action='store_true', default=True,\n",
      "            help='Perform train-time data augmentation (enabled by default)')\n",
      "    parser.add_argument('--no-augment',\n",
      "            action='store_false', dest='augment',\n",
      "            help='Disable train-time data augmentation')\n",
      "    parser.add_argument('--cache-spectra', metavar='DIR',\n",
      "            type=str, default=None,\n",
      "            help='Store spectra in the given directory (disabled by default)')\n",
      "    return parser\n",
      "\n",
      "def main():\n",
      "    # parse command line\n",
      "    parser = opts_parser()\n",
      "    options = parser.parse_args()\n",
      "    modelfile = options.modelfile\n",
      "    sample_rate = 22050\n",
      "    frame_len = 1024\n",
      "    fps = 70\n",
      "    mel_bands = 80\n",
      "    mel_min = 27.5\n",
      "    mel_max = 8000\n",
      "    blocklen = 115\n",
      "    batchsize = 32\n",
      "    \n",
      "    bin_nyquist = frame_len // 2 + 1\n",
      "    bin_mel_max = bin_nyquist * 2 * mel_max // sample_rate\n",
      "\n",
      "    # prepare dataset\n",
      "    datadir = os.path.join(os.path.dirname(__file__),\n",
      "                           os.path.pardir, 'datasets', options.dataset)\n",
      "\n",
      "    # - load filelist\n",
      "    with io.open(os.path.join(datadir, 'filelists', 'train')) as f:\n",
      "        filelist = [l.rstrip() for l in f if l.rstrip()]\n",
      "\n",
      "    # - compute spectra\n",
      "    print(\"Computing%s spectra...\" %\n",
      "          (\" or loading\" if options.cache_spectra else \"\"))\n",
      "    spects = []\n",
      "    for fn in progress(filelist, 'File '):\n",
      "        cache_fn = (options.cache_spectra and\n",
      "                    os.path.join(options.cache_spectra, fn + '.npy'))\n",
      "        spects.append(cached(cache_fn,\n",
      "                             audio.extract_spect,\n",
      "                             os.path.join(datadir, 'audio', fn),\n",
      "                             sample_rate, frame_len, fps))\n",
      "\n",
      "    # - load and convert corresponding labels\n",
      "    print(\"Loading labels...\")\n",
      "    labels = []\n",
      "    for fn, spect in zip(filelist, spects):\n",
      "        fn = os.path.join(datadir, 'labels', fn.rsplit('.', 1)[0] + '.lab')\n",
      "        with io.open(fn) as f:\n",
      "            segments = [l.rstrip().split() for l in f if l.rstrip()]\n",
      "        segments = [(float(start), float(end), label == 'sing')\n",
      "                    for start, end, label in segments]\n",
      "        timestamps = np.arange(len(spect)) / float(fps)\n",
      "        labels.append(create_aligned_targets(segments, timestamps, np.bool))\n",
      "\n",
      "    # - prepare mel filterbank\n",
      "    filterbank = audio.create_mel_filterbank(sample_rate, frame_len, mel_bands,\n",
      "                                             mel_min, mel_max)\n",
      "    filterbank = filterbank[:bin_mel_max].astype(floatX)\n",
      "\n",
      "    # - precompute mel spectra, if needed, otherwise just define a generator\n",
      "    mel_spects = (np.log(np.maximum(np.dot(spect[:, :bin_mel_max], filterbank),\n",
      "                                    1e-7))\n",
      "                  for spect in spects)\n",
      "    if not options.augment:\n",
      "        mel_spects = list(mel_spects)\n",
      "        del spects\n",
      "\n",
      "    # - load mean/std or compute it, if not computed yet\n",
      "    meanstd_file = os.path.join(os.path.dirname(__file__),\n",
      "                                '%s_meanstd.npz' % options.dataset)\n",
      "    try:\n",
      "        with np.load(meanstd_file) as f:\n",
      "            mean = f['mean']\n",
      "            std = f['std']\n",
      "    except (IOError, KeyError):\n",
      "        print(\"Computing mean and standard deviation...\")\n",
      "        mean, std = znorm.compute_mean_std(mel_spects)\n",
      "        np.savez(meanstd_file, mean=mean, std=std)\n",
      "    mean = mean.astype(floatX)\n",
      "    istd = np.reciprocal(std).astype(floatX)\n",
      "\n",
      "    # - prepare training data generator\n",
      "    print(\"Preparing training data feed...\")\n",
      "    if not options.augment:\n",
      "        # Without augmentation, we just precompute the normalized mel spectra\n",
      "        # and create a generator that returns mini-batches of random excerpts\n",
      "        mel_spects = [(spect - mean) * istd for spect in mel_spects]\n",
      "        batches = augment.grab_random_excerpts(\n",
      "            mel_spects, labels, batchsize, blocklen)\n",
      "    else:\n",
      "        # For time stretching and pitch shifting, it pays off to preapply the\n",
      "        # spline filter to each input spectrogram, so it does not need to be\n",
      "        # applied to each mini-batch later.\n",
      "        spline_order = 2\n",
      "        if spline_order > 1:\n",
      "            from scipy.ndimage import spline_filter\n",
      "            spects = [spline_filter(spect, spline_order).astype(floatX)\n",
      "                      for spect in spects]\n",
      "\n",
      "        # We define a function to create the mini-batch generator. This allows\n",
      "        # us to easily create multiple generators for multithreading if needed.\n",
      "        def create_datafeed(spects, labels):\n",
      "            # With augmentation, as we want to apply random time-stretching,\n",
      "            # we request longer excerpts than we finally need to return.\n",
      "            max_stretch = .3\n",
      "            batches = augment.grab_random_excerpts(\n",
      "                    spects, labels, batchsize=batchsize,\n",
      "                    frames=int(blocklen / (1 - max_stretch)))\n",
      "\n",
      "            # We wrap the generator in another one that applies random time\n",
      "            # stretching and pitch shifting, keeping a given number of frames\n",
      "            # and bins only.\n",
      "            max_shift = .3\n",
      "            batches = augment.apply_random_stretch_shift(\n",
      "                    batches, max_stretch, max_shift,\n",
      "                    keep_frames=blocklen, keep_bins=bin_mel_max,\n",
      "                    order=spline_order, prefiltered=True)\n",
      "\n",
      "            # We transform the excerpts to mel frequency and log magnitude.\n",
      "            batches = augment.apply_filterbank(batches, filterbank)\n",
      "            batches = augment.apply_logarithm(batches)\n",
      "\n",
      "            # We apply random frequency filters\n",
      "            batches = augment.apply_random_filters(batches, filterbank,\n",
      "                                                   mel_max, max_db=10)\n",
      "\n",
      "            # We apply normalization\n",
      "            batches = augment.apply_znorm(batches, mean, istd)\n",
      "\n",
      "            return batches\n",
      "\n",
      "        # We start the mini-batch generator and augmenter in one or more\n",
      "        # background threads or processes (unless disabled).\n",
      "        bg_threads = 3\n",
      "        bg_processes = 0\n",
      "        if not bg_threads and not bg_processes:\n",
      "            # no background processing: just create a single generator\n",
      "            batches = create_datafeed(spects, labels)\n",
      "        elif bg_threads:\n",
      "            # multithreading: create a separate generator per thread\n",
      "            batches = augment.generate_in_background(\n",
      "                    [create_datafeed(spects, labels)\n",
      "                     for _ in range(bg_threads)],\n",
      "                    num_cached=bg_threads * 5)\n",
      "        elif bg_processes:\n",
      "            # multiprocessing: single generator is forked along with processes\n",
      "            batches = augment.generate_in_background(\n",
      "                    [create_datafeed(spects, labels)] * bg_processes,\n",
      "                    num_cached=bg_processes * 25,\n",
      "                    in_processes=True)\n",
      "\n",
      "\n",
      "    print(\"Preparing training function...\")\n",
      "    # instantiate neural network\n",
      "    input_var = T.tensor3('input')\n",
      "    inputs = input_var.dimshuffle(0, 'x', 1, 2)  # insert \"channels\" dimension\n",
      "    network = model.architecture(inputs, (None, 1, blocklen, mel_bands))\n",
      "\n",
      "    # create cost expression\n",
      "    target_var = T.vector('targets')\n",
      "    targets = (0.02 + 0.96 * target_var)  # map 0 -> 0.02, 1 -> 0.98\n",
      "    targets = targets.dimshuffle(0, 'x')  # turn into column vector\n",
      "    outputs = lasagne.layers.get_output(network, deterministic=False)\n",
      "    cost = T.mean(lasagne.objectives.binary_crossentropy(outputs, targets))\n",
      "\n",
      "    # prepare and compile training function\n",
      "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
      "    initial_eta = 0.01\n",
      "    eta_decay = 0.85\n",
      "    momentum = 0.95\n",
      "    eta = theano.shared(lasagne.utils.floatX(initial_eta))\n",
      "    updates = lasagne.updates.nesterov_momentum(cost, params, eta, momentum)\n",
      "    print(\"Compiling training function...\")\n",
      "    train_fn = theano.function([input_var, target_var], cost, updates=updates)\n",
      "\n",
      "    # run training loop\n",
      "    print(\"Training:\")\n",
      "    epochs = 20\n",
      "    epochsize = 2000\n",
      "    batches = iter(batches)\n",
      "    for epoch in range(epochs):\n",
      "        err = 0\n",
      "        for batch in progress(\n",
      "                range(epochsize), min_delay=.5,\n",
      "                desc='Epoch %d/%d: Batch ' % (epoch + 1, epochs)):\n",
      "            err += train_fn(*next(batches))\n",
      "            if not np.isfinite(err):\n",
      "                print(\"\\nEncountered NaN loss in training. Aborting.\")\n",
      "                sys.exit(1)\n",
      "        print(\"Train loss: %.3f\" % (err / epochsize))\n",
      "        eta.set_value(eta.get_value() * lasagne.utils.floatX(eta_decay))\n",
      "\n",
      "    # save final network\n",
      "    print(\"Saving final model\")\n",
      "    np.savez(modelfile, **{'param%d' % i: p for i, p in enumerate(\n",
      "            lasagne.layers.get_all_param_values(network))})\n",
      "\n",
      "if __name__==\"__main__\":\n",
      "    main()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q5_check = keyword_search('voice', doc_strings, content)\n",
    "print(q5_check[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
